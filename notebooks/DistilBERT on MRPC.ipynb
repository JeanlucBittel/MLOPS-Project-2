{"cells":[{"cell_type":"markdown","metadata":{"id":"UDR14egIxpP8"},"source":["Finetunes [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on [MRPC](https://huggingface.co/datasets/glue/viewer/mrpc/train). Adapted from a [PyTorch Lightning example](https://lightning.ai/docs/pytorch/1.9.5/notebooks/lightning_examples/text-transformers.html)."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41812,"status":"ok","timestamp":1697542784497,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"nFB7BXYmIdfv","outputId":"cf619a0e-ae7f-49a7-b2af-900bdad0560f"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -q torch transformers pytorch_lightning==1.9.5 datasets\n","%pip install wandb -qU\n","%pip install -q \"ray[tune]\" torchvision"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13716,"status":"ok","timestamp":1697542801291,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"CzaPnn8dL0p3"},"outputs":[],"source":["from datetime import datetime\n","from typing import Optional\n","\n","import wandb\n","import datasets\n","import torch\n","import pytorch_lightning as pl\n","from torch.utils.data import DataLoader\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    get_linear_schedule_with_warmup,\n","    get_inverse_sqrt_schedule,\n","    get_cosine_schedule_with_warmup,\n","    get_polynomial_decay_schedule_with_warmup\n",")\n","from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n","\n","from ray.train.lightning import (\n","    RayDDPStrategy,\n","    RayLightningEnvironment,\n","    RayTrainReportCallback,\n","    prepare_trainer,\n",")\n","\n","from ray import tune\n","from ray.tune.schedulers import ASHAScheduler\n","from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n","from ray.train.torch import TorchTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"elapsed":13207,"status":"ok","timestamp":1697457518151,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"lsteQGcOZ2hN","outputId":"e0b96930-8ca0-4205-c5da-f58914269bb3"},"outputs":[{"data":{"application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1697542804899,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"a4tWVuv_QsGR"},"outputs":[],"source":["class GLUEDataModule(LightningDataModule):\n","    task_text_field_map = {\n","        \"cola\": [\"sentence\"],\n","        \"sst2\": [\"sentence\"],\n","        \"mrpc\": [\"sentence1\", \"sentence2\"],\n","        \"qqp\": [\"question1\", \"question2\"],\n","        \"stsb\": [\"sentence1\", \"sentence2\"],\n","        \"mnli\": [\"premise\", \"hypothesis\"],\n","        \"qnli\": [\"question\", \"sentence\"],\n","        \"rte\": [\"sentence1\", \"sentence2\"],\n","        \"wnli\": [\"sentence1\", \"sentence2\"],\n","        \"ax\": [\"premise\", \"hypothesis\"],\n","    }\n","\n","    glue_task_num_labels = {\n","        \"cola\": 2,\n","        \"sst2\": 2,\n","        \"mrpc\": 2,\n","        \"qqp\": 2,\n","        \"stsb\": 1,\n","        \"mnli\": 3,\n","        \"qnli\": 2,\n","        \"rte\": 2,\n","        \"wnli\": 2,\n","        \"ax\": 3,\n","    }\n","\n","    loader_columns = [\n","        \"datasets_idx\",\n","        \"input_ids\",\n","        \"token_type_ids\",\n","        \"attention_mask\",\n","        \"start_positions\",\n","        \"end_positions\",\n","        \"labels\",\n","    ]\n","\n","    def __init__(\n","        self,\n","        model_name_or_path: str,\n","        task_name: str = \"mrpc\",\n","        max_seq_length: int = 128,\n","        train_batch_size: int = 32,\n","        eval_batch_size: int = 32,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","        self.model_name_or_path = model_name_or_path\n","        self.task_name = task_name\n","        self.max_seq_length = max_seq_length\n","        self.train_batch_size = train_batch_size\n","        self.eval_batch_size = eval_batch_size\n","\n","        self.text_fields = self.task_text_field_map[task_name]\n","        self.num_labels = self.glue_task_num_labels[task_name]\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n","\n","    def setup(self, stage: str):\n","        self.dataset = datasets.load_dataset(\"glue\", self.task_name)\n","\n","        for split in self.dataset.keys():\n","            self.dataset[split] = self.dataset[split].map(\n","                self.convert_to_features,\n","                batched=True,\n","                remove_columns=[\"label\"],\n","            )\n","            self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n","            self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n","\n","        self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n","\n","    def prepare_data(self):\n","        datasets.load_dataset(\"glue\", self.task_name)\n","        AutoTokenizer.from_pretrained(self.model_name_or_path, use_fast=True)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size, shuffle=True)\n","\n","    def val_dataloader(self):\n","        if len(self.eval_splits) == 1:\n","            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n","        elif len(self.eval_splits) > 1:\n","            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n","\n","    def test_dataloader(self):\n","        if len(self.eval_splits) == 1:\n","            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n","        elif len(self.eval_splits) > 1:\n","            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n","\n","    def convert_to_features(self, example_batch, indices=None):\n","        # Either encode single sentence or sentence pairs\n","        if len(self.text_fields) > 1:\n","            texts_or_text_pairs = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n","        else:\n","            texts_or_text_pairs = example_batch[self.text_fields[0]]\n","\n","        # Tokenize the text/text pairs\n","        features = self.tokenizer.batch_encode_plus(\n","            texts_or_text_pairs, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True\n","        )\n","\n","        # Rename label to labels to make it easier to pass to model forward\n","        features[\"labels\"] = example_batch[\"label\"]\n","\n","        return features"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1697542816766,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"M-casw13Qw19"},"outputs":[],"source":["class GLUETransformer(LightningModule):\n","    def __init__(\n","        self,\n","        model_name_or_path: str,\n","        num_labels: int,\n","        task_name: str,\n","        learning_rate: float = 2e-5,\n","        adam_epsilon: float = 1e-8,\n","        warmup_steps: int = 0,\n","        weight_decay: float = 0.0,\n","        train_batch_size: int = 32,\n","        eval_batch_size: int = 32,\n","        eval_splits: Optional[list] = None,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","\n","        self.save_hyperparameters()\n","\n","        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n","        self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n","        self.metric = datasets.load_metric(\n","            \"glue\", self.hparams.task_name, experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n","        )\n","\n","        self.training_step_outputs = []\n","        self.val_step_outputs = []\n","\n","        self.wandb_name = \"learning_rate_1e-5\"\n","        self.epoch = 0\n","\n","        # wandb.init(\n","        #     # Set the project where this run will be logged\n","        #     project=\"MLOPS\",\n","        #     # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","        #     name=f\"{self.wandb_name}_epoch_{self.epoch}\",\n","        #     reinit=True,\n","        #     # Track hyperparameters and run metadata\n","        #     config={\n","        #         \"architecture\": \"DistilBERT\",\n","        #         \"dataset\": \"MRPC\",\n","        #         \"learning_rate\": self.hparams.learning_rate,\n","        #         \"adam_epsilon\": self.hparams.adam_epsilon,\n","        #         \"warmup_steps\": self.hparams.warmup_steps,\n","        #         \"weight_decay\": self.hparams.weight_decay,\n","        #         \"train_batch_size\": self.hparams.train_batch_size,\n","        #         \"eval_batch_size\": self.hparams.eval_batch_size\n","        #     })\n","\n","        # wandb.log({\n","        #     \"learning_rate\": self.hparams.learning_rate,\n","        #     \"adam_epsilon\": self.hparams.adam_epsilon,\n","        #     \"warmup_steps\": self.hparams.warmup_steps,\n","        #     \"weight_decay\": self.hparams.weight_decay,\n","        #     \"train_batch_size\": self.hparams.train_batch_size,\n","        #     \"eval_batch_size\": self.hparams.eval_batch_size\n","        # })\n","\n","    def forward(self, **inputs):\n","        return self.model(**inputs)\n","\n","    def training_step(self, batch, batch_idx):\n","        if self.current_epoch != self.epoch:\n","            self.epoch = self.current_epoch\n","            # wandb.init(\n","            #     project=\"MLOPS\",\n","            #     name=f\"{self.wandb_name}_epoch_{self.epoch}\",\n","            #     config=wandb.config\n","            # )\n","\n","            # wandb.log({\n","            #     \"learning_rate\": self.hparams.learning_rate,\n","            #     \"adam_epsilon\": self.hparams.adam_epsilon,\n","            #     \"warmup_steps\": self.hparams.warmup_steps,\n","            #     \"weight_decay\": self.hparams.weight_decay,\n","            #     \"train_batch_size\": self.hparams.train_batch_size,\n","            #     \"eval_batch_size\": self.hparams.eval_batch_size\n","            # })\n","\n","        outputs = self(**batch)\n","        loss = outputs[0]\n","\n","        metrics = {\n","            \"train/train_loss\": loss\n","        }\n","        # wandb.log(metrics)\n","\n","        self.training_step_outputs.append(loss)\n","        self.log(\"train/train_loss\", loss)\n","\n","        return loss\n","\n","    def on_train_epoch_end(self):\n","        loss = torch.stack(self.training_step_outputs).mean()\n","\n","        metrics = {\n","            \"train/average_train_loss\": loss\n","        }\n","        # wandb.log(metrics)\n","        self.log(\"train/train_average_loss\", loss)\n","\n","        self.training_step_outputs.clear()\n","\n","    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n","        outputs = self(**batch)\n","        val_loss, logits = outputs[:2]\n","\n","        if self.hparams.num_labels > 1:\n","            preds = torch.argmax(logits, axis=1)\n","        elif self.hparams.num_labels == 1:\n","            preds = logits.squeeze()\n","\n","        labels = batch[\"labels\"]\n","\n","        metrics = {\n","            \"val/val_loss\": val_loss\n","        }\n","        # wandb.log(metrics)\n","\n","        self.val_step_outputs.append(val_loss)\n","        self.log(\"val/val_loss\", val_loss)\n","\n","        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels}\n","\n","    def on_validation_epoch_end(self):\n","        loss = torch.stack(self.val_step_outputs).mean()\n","\n","        metrics = {\n","            \"val/average_val_loss\": loss\n","        }\n","        # wandb.log(metrics)\n","        self.log(\"val/val_average_loss\", loss)\n","\n","        self.val_step_outputs.clear()\n","\n","    def validation_epoch_end(self, outputs):\n","        if self.hparams.task_name == \"mnli\":\n","            for i, output in enumerate(outputs):\n","                # matched or mismatched\n","                split = self.hparams.eval_splits[i].split(\"_\")[-1]\n","                preds = torch.cat([x[\"preds\"] for x in output]).detach().cpu().numpy()\n","                labels = torch.cat([x[\"labels\"] for x in output]).detach().cpu().numpy()\n","                loss = torch.stack([x[\"loss\"] for x in output]).mean()\n","                self.log(f\"val_loss_{split}\", loss, prog_bar=True)\n","                split_metrics = {\n","                    f\"{k}_{split}\": v for k, v in self.metric.compute(predictions=preds, references=labels).items()\n","                }\n","                self.log_dict(split_metrics, prog_bar=True)\n","            return loss\n","\n","        preds = torch.cat([x[\"preds\"] for x in outputs]).detach().cpu().numpy()\n","        labels = torch.cat([x[\"labels\"] for x in outputs]).detach().cpu().numpy()\n","        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","\n","        metrics = {\n","            \"val/average_val_loss\": loss\n","        }\n","        # wandb.log(metrics)\n","\n","        self.log(\"val_loss\", loss, prog_bar=True)\n","        self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n","\n","    def configure_optimizers(self):\n","        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","\n","        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","        # optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=self.hparams.learning_rate, momentum=1)\n","        # optimizer = torch.optim.ASGD(optimizer_grouped_parameters, lr=self.hparams.learning_rate, lambd=0.000001)\n","\n","        # scheduler = get_inverse_sqrt_schedule(\n","        # scheduler = get_cosine_schedule_with_warmup(\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=self.hparams.warmup_steps,\n","            num_training_steps=self.trainer.estimated_stepping_batches,\n","            # timescale=1\n","        )\n","        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n","        return [optimizer], [scheduler]\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":842,"referenced_widgets":["8e7fa04a1a754856a1ead791668be729","c7db178d3e084778903eb652b625d95f","4127317775ea4b16a4306dab6a2487e3","62e33bb38880471db7189c1b63574a09","e612dbde6c364ee393fc1c2a2fd74691","df1e2b706a9d44eb852cc2d4c0834daa","33952ceb871a46b9bea551be8f9e10b6","65da10b3fa8e4ad8b867d9e671c94bc2","41e1e517b1e44d7da2fe6e410d7974ab","9dd73616187e4d83816e2b6276c4107d","14728ac7e9be416bbd557dddf904353b","ee3aea985de24274902c0c35b7c9a486","39df911a0eac45c78e918d4b387920a5","bf7bf27a35ef4dc2b20612017d2f06a1","80ed685a43c74043af45f533570b4f29","9be79ef6d5144015bec0e70a5d93f349","ffb2baec7254432f996a45d63db60b57","bb78b3609c69474090087ee75cff3dce","da01b07b55044e6bb220ae78920f1158","02e3487391a5467686ac798e17f0c7c9","82fca981fb3641599152bb53030a0286","663099f2b90b43a7967a588f477fa2a6","6a2ed32a0a12459d8ec8ccbd2e5507c5","d2280552615a4187a06f7e4763f5507d","f105a370e5774d26b2231b21245bdf7c","3c07904504e14a4aa99e2a2ccbbc0448","400e11e4b0a54ce0a152381c285bc1c7","e13362715ba54177ac9465a998984ea7","2701b457023b4edf80ccc6a98c5ef08d","8fa2309779064a5b8df947dac5d0f8cd","8bf3c552723e47db8c4e1b3b5b548a36","85f178a458924a0eadb3bb3549bfffae","380f9e66449841249a422397cfc51fab","4f4f106482fb4991a9017c5871492d8e","fc6492174fe1495990dd47eaace50d52","10851459e221472296e92b69f1ffdeb8","9a899e01e8234ecf9a3b96483f564f6d","30aeb0512c30413eb8c34e81dd141029","f838dc6ff69b41c682df2af6cc8fd569","431e656e24464372ab6fca6724452862","43c18ffb110441b7899f1e1e80bfc8d8","a56d9d332dcd4c75944c1cfc3bf182a0","4bebfda1fb1248e1a60964dea6b93865","f3fe2799a6d64c6ab23492b68f3ee569","f77ee1e584c145f3bd2a562b8c958e01","5a2695e21c3c485b884c1e35e3ca1bcf","9783f588320044059ef1d1339d9f306d","711e7b223bfa4745840bcf97707ae79e","b9851cf025894e5e9d1303d738d85dcf","aeef2870a1084bcd9b85ecb7163b3d3a","0aec5670730744fd9d3a18a361dd4ff7","b522e13c07b446678e9c0850ac7d8c46","6a0390e51455445aa3a85e87ca17abc7","b4db7766446e42a8b54fbfab637a868d","9603c60a79064c3f8e7a58d732cba98d","7ec5dcf538cc41a0ae1d30109b3ee552","6b479e848c55474797569d48bcce50f0","1578e896c7b84d04aef3aa6f5acbeeca","c7016f78951649828d54b826c39d67f7","0f56be71d76b47838f65871a83785a5f","e5591dd1654c43ed805f2cfa014d791c","004bc5e0b68d4b3f9f2939648092bfee","6189683c97e643f2830bf915a573b899","be6aee17c20640ed95586fe17013b631","07bd9d9ac50e42dbb60fa8a91a0ba403","9d27c79c46d74bb5bba5d3fd280fce30","ae22dfc3b6b243fc95660b09b173f00b","166bb8fddc27442b9ccf1018a7a7715f","ed252b47069a496a98595dba61c13750","c9a0c4fc0057459d918c12294b08697b","a31b4d419786431da75b7cb13d832f14","8b0f7aacd5e64c9c8000a9f9466c5a0e","c966d0f0f9834198bd362bbb6c816131","eeac8b2847d74b35a1ff367be762424f","3133415bebd147c6a141ada14746312c","026b743ebaec4d5ebb1838b6485b01bc","25b1b1dd39854d39b12c850142c2785d","66920651c5ed4d208788bbd42e18bf40","8e765e1455a24f6aa516399fd9f1c886","ade943ec1c7f47cb82da5be6089e4fb4","96aa19d46ae34fd299e3f5675bfa6524","bbf40cc487de4e6fbc9ee7271375aa3a","75aca36e6df942cca5052dcaa057bf06","62e6cc9ad1024590ab26a52d4fb45199","1cc2ff419bb5438fb2778b52e203052a","14c56fb1941e43cdb8adc486817f07bc","ece466e7c8ad4aa384fc516b02be974c","6e7aed5d24f8444aaf66b080edc5ef35","599d5f2098554c4e95aed23e3fa03026","85883d5affe7419c9b8d7bf07e112f12","cdbe22a341ec4cfc9f0006049080f844","43b185552b07440bab7946a3a7c4fd99","d34133e54dfc4bccb0d43303f2b4a251","7241997396bc438d9c87a300d3db344f","09fb4d73fc834b0a92359db3f9994ec3","e6999c2164ac4b80be1e7338e8ad611b","65ce81a81f9241838de17ce8e53504ad","84a669308a114c2da442b961986fdb99","a3765ee2b0e84fb3b92fa0b657e9d45a","92116bcc61004dfba7412cc9b4becc26","66512a6f542d4d0c9836da12c83b56f1","cf03ba3139d84f7eaf7d4cda9bf8f600","c6f25620bf114bf7860e6b5012b37100","eeac69648fe4416ab93eab8286f8d1c2","32d70ddf17b54066821f648f90883ca5","24611bdc999648fc834d3868ccf629fd","a32ee8ad7a4641d786902d681160c2fa","07ee38b3498d42f69ede7fb3e8708118","dd29881de199432caefc8beafffc17e2","7afeeb7e3fb64861bf994c94c168014e","7f74a5f616cf48bf86ec8983ced66d20","af7530e3685a4f10a1f026713af2b0f4","3b0f7a1c720e495488fa395fa796474c","0b0655faf264472785c7041e08534654","69781278612b455cbf6d5c94f25feced","68a0dc2cf61f4f40af9b539ec1c45536","74f44c22eddd42a7a8afb4b58a4218ac","d502bf65b7c24600a0d4a9d85130b05e","710f97a41f364d169cf7077cd6b5cc1e","bfc085b6deea454cad3f9d46ee6f429c","cfa3e95e3b3d4b679aa09d5cf13a5ec9","637f22bb7ef64f07ad4856a6bc469a54","458085ad133e451fbd334ea65a588923","22ddde21e3fe47fe92e50c96052d60aa","255506ff5107439abd85b0678a12232b","3fa8a934cb04414ea96b493f804e0524","7de77b034bf64113bcfb011b1aed5aeb","069d2f12ddcf4645bbfb7f30078c847a","b680c5211fbb432d9a159462e5002d52","21a59f4ada4840c1bc9268d234a8e160","3ea9a141e33a4a26a86b46c564864556","4132989cb9d1483a96c475fdcd36c243","3810b0e869bf416cbfa85ca91bd8ac40","5d216d7d167840568c2c66f4e44597f2","d6f39e6f87894457b5afa446ec8aa8f1","25cee06da5b74f14b137c12c1f7cb670","ad83b367ed6b4069926e86c5150297ce","a9542f8330de42679fec1e7e5a463883","e62e4acd374d4838ac141b32235dfa12","1bdf7c6b193441b3a3d5e4b3af4b815d","3ecad6524fff4e24b1e0ee8f4c2013b9","f3077d90b8764186a245e34fd2399b07","681b767394f9430899a0b1e215851b06","2d7070ecd36a4968801c25d181a006b8","84517d98e2ad4d86ae40030bbef86b56","4f7e3c6c9d2c45bfbee63d95335400f5","4586527c328e4ae397bde1bfe8b9a59e","32a72319303b4c738c179672fe15f8db","d4c12c33cf504dec80a1a7acd194a945","736b774e9609422cb4b50635de7c554f","dea8c0f8d40d4a24b5d722eae4aacce7","163975a641254219b917214ebe1841c1","526a174ea90d4842a7d0358356cdfa9d","a22216cbb3cc44adabbf253bfae31434","463a0e09f3044219ae13a0b88709f97b","0ed84888dc6842fe896e9a3a67288257","121cdf2eb5fa41c8abc02bc64d8a15d8","854a7a6a4a1b4bd0b9440170796e6351","4f01dc7d37ae4cdda5aaa1cd89221fee","fed062a5c9604a6c861a4ee89929d77c","2b383ba2b4d04598ae9e21cd086e963d","c093365d36304e018fbd305a3a66ba71","598bb4d4baa1419e81c95cb9072bbab1","71a1db5806a94eacbd9a106117b955e1","13676e154a0245b7a42d660c2acefe74","532385c63a154c029190d1bc189ef4f2","93f735e4079c4ddfa064efce6fb554d4","3ffb98fa0f4f4845a3e6f4df529c44c8","3039526144e548caab131a25789b82b0","d580006fa1d84e04a65bbc31db5675d4","5dd1de1bab5c4dc392069e62d0dcec9b","9562eaeb7db04a46829e31ec866890bb","cbf96db2943e4e6c8220770f9d4bbe7e","40d8d87016f142f1af91e53eebff98af","cdd7ef365a0543928fd04e141acfb49d","a865a8cc363147029008ad76c02b707b","25d2de16b5b049618597c5f40cb5970a","d99576503ba64499a86048f1428f2e77","95475e1e578b42848b7b56196aaccb49","4c79638e1c664e6ca7ada2d1faa034b0","4cd0b778bd7c489c9ecb3d56a43af724","ff934dd0990d4bd0a873cd88c3fb8a2e","ab2a03644a52440d9c3f118301a06277","01f243c6f207448999be9f108666d0a0","3a7783fb4e33468ca2d92dd752983f0d","1beeff4a30b744b193ae46d8d6b588a4","efed7fe224594a4985c3fac77b5a8aba","1f445e7cfff44e9fbaa1459eff2ce121","e44c7e8818d540fea169feff2edaf66a","6e4bb9727ddb445e98e3827dbf4f7993","ab0bd56fd10d4e6194871a1b59c78b70","ab5725e298c04d67ad70fd3073a0c420","01f7a14072944f508e18c4be851f8d05","2ccf61e37cf645feb2552df01f4ca269","4857cd3df9514f0b8875357717623ef4","f8a1b360cbda4e9e961e45fbdc8a235b","17a68c93c6e94f5b810d9f21673a7114","1acff480844e4fb9a684263a340fcf48","f679b0829856491aac7a1ac55b7cec9f","c89b358605f54e21bc0bd5ccea02b108","69b55c5673f34162a40004b6f5b4ceeb","7194484f386c4b95880b365ea752baf5","8cee5a85e26542358ea241e769a30ddd","c626803332984fddac407d524b8d97ff","df065d7e0a6d4f079148f969d2bafe15","b42b798910dd4441bd3a06ce6e4910f6","e9454384e86b4ab9be685652afc19990","91373c6d3278470286c1e67ada83054d","9dd81dbb0ad74f6fbc5482a9375fd6a3"]},"executionInfo":{"elapsed":13300,"status":"ok","timestamp":1697542842948,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"tGW_9OuzQ1JF","outputId":"fdb95db9-acc1-4505-c782-78d2b30b01d1"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:lightning_fabric.utilities.seed:Global seed set to 42\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e7fa04a1a754856a1ead791668be729","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee3aea985de24274902c0c35b7c9a486","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a2ed32a0a12459d8ec8ccbd2e5507c5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f4f106482fb4991a9017c5871492d8e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f77ee1e584c145f3bd2a562b8c958e01","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ec5dcf538cc41a0ae1d30109b3ee552","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae22dfc3b6b243fc95660b09b173f00b","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66920651c5ed4d208788bbd42e18bf40","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"599d5f2098554c4e95aed23e3fa03026","version_major":2,"version_minor":0},"text/plain":["Downloading data: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92116bcc61004dfba7412cc9b4becc26","version_major":2,"version_minor":0},"text/plain":["Downloading data: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7f74a5f616cf48bf86ec8983ced66d20","version_major":2,"version_minor":0},"text/plain":["Downloading data: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"637f22bb7ef64f07ad4856a6bc469a54","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3810b0e869bf416cbfa85ca91bd8ac40","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d7070ecd36a4968801c25d181a006b8","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"463a0e09f3044219ae13a0b88709f97b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"532385c63a154c029190d1bc189ef4f2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/408 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"25d2de16b5b049618597c5f40cb5970a","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f445e7cfff44e9fbaa1459eff2ce121","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","<ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  self.metric = datasets.load_metric(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f679b0829856491aac7a1ac55b7cec9f","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]}],"source":["seed_everything(42)\n","\n","dm = GLUEDataModule(\n","    model_name_or_path=\"distilbert-base-uncased\",\n","    task_name=\"mrpc\",\n","    # train_batch_size=64,\n","    # eval_batch_size=64\n",")\n","dm.setup(\"fit\")\n","model = GLUETransformer(\n","    model_name_or_path=\"distilbert-base-uncased\",\n","    num_labels=dm.num_labels,\n","    eval_splits=dm.eval_splits,\n","    task_name=dm.task_name,\n","    # learning_rate=1e-5,\n","    # warmup_steps=75,\n","    # train_batch_size=64,\n","    # eval_batch_size=64,\n","    # weight_decay=1e-5,\n","    # adam_epsilon=1e-10,\n",")\n","\n","trainer = Trainer(\n","    max_epochs=3,\n","    accelerator=\"auto\",\n","    devices=1 if torch.cuda.is_available() else None,\n",")\n","# trainer.fit(model, datamodule=dm)\n","\n","# wandb.finish()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1697542847491,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"NPCv0wKxSVpz"},"outputs":[],"source":["def train_func(config):\n","    dm2 = GLUEDataModule(\n","        model_name_or_path=\"distilbert-base-uncased\",\n","        task_name=\"mrpc\",\n","        train_batch_size=config[\"batch_size\"],\n","        eval_batch_size=config[\"batch_size\"]\n","    )\n","    dm2.setup(\"fit\")\n","    model = GLUETransformer(\n","        model_name_or_path=\"distilbert-base-uncased\",\n","        num_labels=dm2.num_labels,\n","        task_name=dm2.task_name,\n","        learning_rate=config[\"learning_rate\"],\n","        warmup_steps=config[\"warmup_steps\"],\n","        train_batch_size=config[\"batch_size\"],\n","        eval_batch_size=config[\"batch_size\"],\n","        eval_splits=dm2.eval_splits,\n","    )\n","\n","    trainer = pl.Trainer(\n","        devices=\"auto\",\n","        accelerator=\"auto\",\n","        strategy=RayDDPStrategy(),\n","        callbacks=[RayTrainReportCallback()],\n","        plugins=[RayLightningEnvironment()],\n","        enable_progress_bar=False,\n","    )\n","    trainer = prepare_trainer(trainer)\n","    trainer.fit(model, datamodule=dm2)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":249,"status":"ok","timestamp":1697542953632,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"XBhyM6CtOZ7f"},"outputs":[],"source":["num_epochs = 3\n","num_samples = 10\n","\n","search_space = {\n","    \"learning_rate\": tune.loguniform(1e-6, 1e-1),\n","    \"batch_size\": tune.choice([4, 8, 16]),\n","    \"warmup_steps\": tune.randint(0, 251)\n","}\n","\n","scaling_config = ScalingConfig(\n","    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",")\n","\n","run_config = RunConfig(\n","    checkpoint_config=CheckpointConfig(\n","        num_to_keep=2,\n","        checkpoint_score_attribute=\"val_loss\",\n","        checkpoint_score_order=\"min\",\n","    ),\n",")\n","\n","ray_trainer = TorchTrainer(\n","    train_func,\n","    run_config=run_config,\n","    scaling_config=scaling_config,\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3580063,"status":"ok","timestamp":1697546535691,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"CgGnq5CfSg4v","outputId":"4e89c177-4c65-4bc3-e5eb-7fdb533813b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-10-17 11:42:37,837\tINFO worker.py:1642 -- Started a local Ray instance.\n","2023-10-17 11:42:40,978\tINFO tune.py:228 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n","2023-10-17 11:42:40,990\tINFO tune.py:654 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"]},{"name":"stdout","output_type":"stream","text":["+---------------------------------------------------------------------+\n","| Configuration for experiment     TorchTrainer_2023-10-17_11-42-32   |\n","+---------------------------------------------------------------------+\n","| Search algorithm                 BasicVariantGenerator              |\n","| Scheduler                        AsyncHyperBandScheduler            |\n","| Number of trials                 25                                 |\n","+---------------------------------------------------------------------+\n","\n","View detailed results here: /root/ray_results/TorchTrainer_2023-10-17_11-42-32\n","To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/TorchTrainer_2023-10-17_11-42-32`\n","\n","Trial status: 16 PENDING\n","Current time: 2023-10-17 11:42:41. Total running time: 0s\n","Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:None)\n","+----------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps |\n","+----------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   PENDING               7.45934e-05                        4                       14 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121 |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99 |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1 |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191 |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21 |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58 |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189 |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243 |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134 |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59 |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52 |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171 |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80 |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133 |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190 |\n","+----------------------------------------------------------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00000 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00000 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        4 |\n","| train_loop_config/learning_rate           7.45934e-05 |\n","| train_loop_config/warmup_steps                     14 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=1684)\u001b[0m Starting distributed worker processes: ['1807 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2465.90 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2641.84 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2559.83 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2545.65 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2441.99 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2502.67 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2516.96 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2461.57 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:43:11. Total running time: 30s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","+----------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps |\n","+----------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121 |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99 |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1 |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191 |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21 |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58 |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189 |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243 |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134 |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59 |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52 |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171 |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80 |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133 |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190 |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189 |\n","+----------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00000_0_batch_size=4,learning_rate=0.0001,warmup_steps=14_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4852.25 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4890.60 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4916.50 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4810.43 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4379.39 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4848.78 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4803.73 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m 2023-10-17 11:43:25.901005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:43:41. Total running time: 1min 0s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","+----------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps |\n","+----------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121 |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99 |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1 |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191 |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21 |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58 |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189 |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243 |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134 |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59 |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52 |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171 |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80 |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133 |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190 |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189 |\n","+----------------------------------------------------------------------------------------------------------------+\n","Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:44:11. Total running time: 1min 30s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","+----------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps |\n","+----------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121 |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99 |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1 |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191 |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21 |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58 |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189 |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243 |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134 |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59 |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52 |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171 |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80 |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133 |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190 |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189 |\n","+----------------------------------------------------------------------------------------------------------------+\n","Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:44:41. Total running time: 2min 0s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","+----------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps |\n","+----------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121 |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99 |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1 |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191 |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21 |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58 |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189 |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243 |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134 |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59 |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52 |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171 |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80 |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133 |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190 |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189 |\n","+----------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00000_0_batch_size=4,learning_rate=0.0001,warmup_steps=14_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:45:12. Total running time: 2min 30s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5556000471115112 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14        1             105.28             0.170858           0.5556       0.5556     0.710784 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189                                                                                             |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:45:42. Total running time: 3min 0s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5556000471115112 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14        1             105.28             0.170858           0.5556       0.5556     0.710784 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189                                                                                             |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00000_0_batch_size=4,learning_rate=0.0001,warmup_steps=14_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:46:12. Total running time: 3min 30s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5041380524635315 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14        2            193.412              0.41176         0.504138     0.504138     0.769608 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189                                                                                             |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:46:42. Total running time: 4min 0s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5041380524635315 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14        2            193.412              0.41176         0.504138     0.504138     0.769608 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189                                                                                             |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:47:12. Total running time: 4min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5041380524635315 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status       ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   RUNNING               7.45934e-05                        4                       14        2            193.412              0.41176         0.504138     0.504138     0.769608 |\n","| TorchTrainer_4753d_00001   PENDING               0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00002   PENDING               1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING               0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING               0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING               0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING               1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING               1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING               0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING               1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING               2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING               8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING               1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING               1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING               0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING               0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING               0.000177325                        8                      189                                                                                             |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=1807)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00000_0_batch_size=4,learning_rate=0.0001,warmup_steps=14_2023-10-17_11-42-41/checkpoint_000002)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00000 completed after 3 iterations at 2023-10-17 11:47:26. Total running time: 4min 45s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00000 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000002 |\n","| time_this_iter_s                                   74.86369 |\n","| time_total_s                                      268.27617 |\n","| training_iteration                                        3 |\n","| accuracy                                            0.72794 |\n","| epoch                                                     2 |\n","| f1                                                  0.79252 |\n","| step                                                   2751 |\n","| train/train_average_loss                            0.43009 |\n","| train/train_loss                                    0.18407 |\n","| val/val_average_loss                                0.54132 |\n","| val/val_loss                                        0.54132 |\n","| val_loss                                            0.54132 |\n","+-------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00001 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00001 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                       16 |\n","| train_loop_config/learning_rate           0.000984674 |\n","| train_loop_config/warmup_steps                    121 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=2994)\u001b[0m Starting distributed worker processes: ['3058 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 1 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:47:42. Total running time: 5min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00001   RUNNING                 0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3            268.276             0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00002   PENDING                 1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2172.56 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2176.97 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2345.48 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2308.59 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2443.66 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2479.74 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2445.53 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00001_1_batch_size=16,learning_rate=0.0010,warmup_steps=121_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4753.28 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4700.64 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4734.09 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4713.39 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4635.87 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4437.12 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 3412.45 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 2023-10-17 11:48:02.208261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 1 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:48:12. Total running time: 5min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00001   RUNNING                 0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3            268.276             0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00002   PENDING                 1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 1 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:48:42. Total running time: 6min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00001   RUNNING                 0.000984674                       16                      121                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3            268.276             0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00002   PENDING                 1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00001 completed after 1 iterations at 2023-10-17 11:48:55. Total running time: 6min 14s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00001 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   80.72773 |\n","| time_total_s                                       80.72773 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    230 |\n","| train/train_loss                                    0.38001 |\n","| val/val_average_loss                                0.62765 |\n","| val/val_loss                                         0.6266 |\n","| val_loss                                            0.62765 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00001_1_batch_size=16,learning_rate=0.0010,warmup_steps=121_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00002 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00002 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        4 |\n","| train_loop_config/learning_rate           1.95172e-06 |\n","| train_loop_config/warmup_steps                     99 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=3455)\u001b[0m Starting distributed worker processes: ['3519 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warnings.warn(\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4049.49 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4789.48 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4630.21 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:49:12. Total running time: 6min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00002_2_batch_size=4,learning_rate=0.0000,warmup_steps=99_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 2772.30 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 3322.85 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 3815.78 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 3675.99 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4528.86 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4714.57 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4567.21 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m 2023-10-17 11:49:26.586060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:49:42. Total running time: 7min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:50:12. Total running time: 7min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:50:42. Total running time: 8min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99        1            98.4298             0.223329         0.554727     0.554727     0.703431 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00002_2_batch_size=4,learning_rate=0.0000,warmup_steps=99_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:51:12. Total running time: 8min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99        1            98.4298             0.223329         0.554727     0.554727     0.703431 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:51:42. Total running time: 9min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00000 with val_loss=0.5413188934326172 and params={'train_loop_config': {'learning_rate': 7.45934328572655e-05, 'batch_size': 4, 'warmup_steps': 14}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99        1            98.4298             0.223329         0.554727     0.554727     0.703431 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00002_2_batch_size=4,learning_rate=0.0000,warmup_steps=99_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:52:12. Total running time: 9min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.46388867497444153 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99        2           179.669              0.2421           0.463889     0.463889     0.781863 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:52:42. Total running time: 10min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.46388867497444153 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99        2           179.669              0.2421           0.463889     0.463889     0.781863 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 2 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:53:13. Total running time: 10min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.46388867497444153 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00002   RUNNING                 1.95172e-06                        4                       99        2           179.669              0.2421           0.463889     0.463889     0.781863 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=3519)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00002_2_batch_size=4,learning_rate=0.0000,warmup_steps=99_2023-10-17_11-42-41/checkpoint_000002)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00002 completed after 3 iterations at 2023-10-17 11:53:33. Total running time: 10min 51s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00002 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000002 |\n","| time_this_iter_s                                   89.27856 |\n","| time_total_s                                      268.94711 |\n","| training_iteration                                        3 |\n","| accuracy                                            0.82843 |\n","| epoch                                                     2 |\n","| f1                                                  0.88255 |\n","| step                                                   2751 |\n","| train/train_average_loss                            0.52208 |\n","| train/train_loss                                    0.52633 |\n","| val/val_average_loss                                0.40604 |\n","| val/val_loss                                        0.40604 |\n","| val_loss                                            0.40604 |\n","+-------------------------------------------------------------+\n","\n","Trial status: 3 TERMINATED | 16 PENDING\n","Current time: 2023-10-17 11:53:43. Total running time: 11min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   PENDING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00003 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00003 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                       4 |\n","| train_loop_config/learning_rate           0.00179656 |\n","| train_loop_config/warmup_steps                     1 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=4718)\u001b[0m Starting distributed worker processes: ['4786 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   warnings.warn(\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4407.73 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4316.20 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00003_3_batch_size=4,learning_rate=0.0018,warmup_steps=1_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4945.84 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4753.30 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4641.67 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4642.72 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4730.47 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 5059.91 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4791.54 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m 2023-10-17 11:54:06.265137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 3 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:54:13. Total running time: 11min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00003   RUNNING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 3 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:54:43. Total running time: 12min 1s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00003   RUNNING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","Trial status: 3 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:55:13. Total running time: 12min 31s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00003   RUNNING                 0.00179656                         4                        1                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00004   PENDING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00003 completed after 1 iterations at 2023-10-17 11:55:18. Total running time: 12min 37s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00003 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   94.86386 |\n","| time_total_s                                       94.86386 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    917 |\n","| train/train_loss                                    0.36489 |\n","| val/val_average_loss                                0.63763 |\n","| val/val_loss                                        0.63763 |\n","| val_loss                                            0.63763 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=4786)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00003_3_batch_size=4,learning_rate=0.0018,warmup_steps=1_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00004 started with configuration:\n","+-----------------------------------------------------+\n","| Trial TorchTrainer_4753d_00004 config               |\n","+-----------------------------------------------------+\n","| train_loop_config/batch_size                      8 |\n","| train_loop_config/learning_rate           0.0492905 |\n","| train_loop_config/warmup_steps                  191 |\n","+-----------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=5245)\u001b[0m Starting distributed worker processes: ['5301 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2333.48 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2743.73 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2724.92 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2663.44 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2289.44 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 3101.21 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2776.36 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00004_4_batch_size=8,learning_rate=0.0493,warmup_steps=191_2023-10-17_11-42-41/lightning_logs\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 4 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:55:43. Total running time: 13min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00004   RUNNING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m \rMap:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4758.63 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4727.95 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4759.28 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4679.28 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4545.70 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 5219.08 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4891.08 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m 2023-10-17 11:55:48.909256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 4 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:56:13. Total running time: 13min 32s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00004   RUNNING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more PENDING\n","Trial status: 4 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:56:43. Total running time: 14min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00004   RUNNING                 0.0492905                          8                      191                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00005   PENDING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more PENDING\n","\n","Trial TorchTrainer_4753d_00004 completed after 1 iterations at 2023-10-17 11:56:50. Total running time: 14min 9s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00004 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   85.70076 |\n","| time_total_s                                       85.70076 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    459 |\n","| train/train_loss                                    0.36582 |\n","| val/val_average_loss                                0.62474 |\n","| val/val_loss                                        0.62474 |\n","| val_loss                                            0.62474 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=5301)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00004_4_batch_size=8,learning_rate=0.0493,warmup_steps=191_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00005 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00005 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                       8 |\n","| train_loop_config/learning_rate           0.00122295 |\n","| train_loop_config/warmup_steps                    21 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=5720)\u001b[0m Starting distributed worker processes: ['5780 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   warnings.warn(\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2026.13 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2652.22 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2565.34 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00005_5_batch_size=8,learning_rate=0.0012,warmup_steps=21_2023-10-17_11-42-41/lightning_logs\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 5 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:57:13. Total running time: 14min 32s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00005   RUNNING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m \rMap:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4853.83 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4796.33 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4841.01 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4729.86 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4612.78 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2436.89 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2364.34 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m 2023-10-17 11:57:23.111519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 5 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:57:43. Total running time: 15min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00005   RUNNING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more PENDING\n","Trial status: 5 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:58:13. Total running time: 15min 32s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00005   RUNNING                 0.00122295                         8                       21                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00006   PENDING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=5780)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00005_5_batch_size=8,learning_rate=0.0012,warmup_steps=21_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00005 completed after 1 iterations at 2023-10-17 11:58:22. Total running time: 15min 40s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00005 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                    84.0109 |\n","| time_total_s                                        84.0109 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    459 |\n","| train/train_loss                                    0.29225 |\n","| val/val_average_loss                                0.64492 |\n","| val/val_loss                                        0.64492 |\n","| val_loss                                            0.64492 |\n","+-------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00006 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00006 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                      16 |\n","| train_loop_config/learning_rate           1.3041e-06 |\n","| train_loop_config/warmup_steps                    58 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=6191)\u001b[0m Starting distributed worker processes: ['6247 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warnings.warn(\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2214.73 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2320.40 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2480.25 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00006_6_batch_size=16,learning_rate=0.0000,warmup_steps=58_2023-10-17_11-42-41/lightning_logs\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 6 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:58:43. Total running time: 16min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00006   RUNNING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","1 more TERMINATED, 11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m \rMap:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 3583.25 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4133.68 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 3996.77 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4032.55 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4748.40 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4790.75 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4711.65 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4669.83 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m 2023-10-17 11:58:51.217965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 6 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:59:13. Total running time: 16min 32s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00006   RUNNING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","1 more TERMINATED, 11 more PENDING\n","Trial status: 6 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 11:59:44. Total running time: 17min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00006   RUNNING                 1.3041e-06                        16                       58                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","1 more TERMINATED, 11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00006_6_batch_size=16,learning_rate=0.0000,warmup_steps=58_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 6 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 12:00:14. Total running time: 17min 32s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00006   RUNNING                 1.3041e-06                        16                       58        1            75.9072             0.344725         0.605391     0.606175     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","1 more TERMINATED, 11 more PENDING\n","\n","Trial TorchTrainer_4753d_00006 completed after 2 iterations at 2023-10-17 12:00:41. Total running time: 18min 0s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00006 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000001 |\n","| time_this_iter_s                                   57.23472 |\n","| time_total_s                                      133.14191 |\n","| training_iteration                                        2 |\n","| accuracy                                            0.68873 |\n","| epoch                                                     1 |\n","| f1                                                   0.8146 |\n","| step                                                    460 |\n","| train/train_average_loss                            0.63906 |\n","| train/train_loss                                    0.51975 |\n","| val/val_average_loss                                0.58355 |\n","| val/val_loss                                         0.5834 |\n","| val_loss                                            0.58355 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=6247)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00006_6_batch_size=16,learning_rate=0.0000,warmup_steps=58_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 7 TERMINATED | 16 PENDING\n","Current time: 2023-10-17 12:00:44. Total running time: 18min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00007   PENDING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","2 more TERMINATED, 11 more PENDING\n","\n","Trial TorchTrainer_4753d_00007 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00007 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                       16 |\n","| train_loop_config/learning_rate           1.71131e-06 |\n","| train_loop_config/warmup_steps                    189 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=6866)\u001b[0m Starting distributed worker processes: ['6934 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warnings.warn(\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4595.65 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4645.15 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00007_7_batch_size=16,learning_rate=0.0000,warmup_steps=189_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 3056.47 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2678.91 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2674.01 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2628.78 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2447.21 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4797.74 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4626.05 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m 267.820   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 7 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 12:01:14. Total running time: 18min 32s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00007   RUNNING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","2 more TERMINATED, 11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m 2023-10-17 12:01:15.674247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 7 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 12:01:44. Total running time: 19min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00007   RUNNING                 1.71131e-06                       16                      189                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","2 more TERMINATED, 11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00007_7_batch_size=16,learning_rate=0.0000,warmup_steps=189_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 7 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 12:02:14. Total running time: 19min 32s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00007   RUNNING                 1.71131e-06                       16                      189        1            76.3051             0.371212         0.605931     0.606573     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","2 more TERMINATED, 11 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 7 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 12:02:44. Total running time: 20min 2s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00007   RUNNING                 1.71131e-06                       16                      189        1            76.3051             0.371212         0.605931     0.606573     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","2 more TERMINATED, 11 more PENDING\n","\n","Trial TorchTrainer_4753d_00007 completed after 2 iterations at 2023-10-17 12:03:09. Total running time: 20min 27s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00007 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000001 |\n","| time_this_iter_s                                   61.52107 |\n","| time_total_s                                      137.82615 |\n","| training_iteration                                        2 |\n","| accuracy                                            0.70588 |\n","| epoch                                                     1 |\n","| f1                                                  0.82301 |\n","| step                                                    460 |\n","| train/train_average_loss                            0.64223 |\n","| train/train_loss                                    0.45895 |\n","| val/val_average_loss                                  0.565 |\n","| val/val_loss                                        0.56544 |\n","| val_loss                                              0.565 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=6934)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00007_7_batch_size=16,learning_rate=0.0000,warmup_steps=189_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 8 TERMINATED | 16 PENDING\n","Current time: 2023-10-17 12:03:14. Total running time: 20min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00008   PENDING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","3 more TERMINATED, 11 more PENDING\n","\n","Trial TorchTrainer_4753d_00008 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00008 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                      16 |\n","| train_loop_config/learning_rate           0.00123575 |\n","| train_loop_config/warmup_steps                   243 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=7571)\u001b[0m Starting distributed worker processes: ['7625 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00008_8_batch_size=16,learning_rate=0.0012,warmup_steps=243_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4599.61 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4576.72 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4316.53 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 3779.08 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 3998.35 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2272.20 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2583.83 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2548.63 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m 2023-10-17 12:03:40.634696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 8 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 12:03:44. Total running time: 21min 3s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00008   RUNNING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","3 more TERMINATED, 11 more PENDING\n","Trial status: 8 TERMINATED | 1 RUNNING | 16 PENDING\n","Current time: 2023-10-17 12:04:14. Total running time: 21min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00008   RUNNING                 0.00123575                        16                      243                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00009   PENDING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","3 more TERMINATED, 11 more PENDING\n","\n","Trial TorchTrainer_4753d_00008 completed after 1 iterations at 2023-10-17 12:04:36. Total running time: 21min 55s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00008 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   81.42005 |\n","| time_total_s                                       81.42005 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    230 |\n","| train/train_loss                                     0.3376 |\n","| val/val_average_loss                                0.64455 |\n","| val/val_loss                                        0.64318 |\n","| val_loss                                            0.64455 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=7625)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00008_8_batch_size=16,learning_rate=0.0012,warmup_steps=243_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00009 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00009 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                       16 |\n","| train_loop_config/learning_rate           1.70707e-06 |\n","| train_loop_config/warmup_steps                    134 |\n","+-------------------------------------------------------+\n","\n","Trial status: 9 TERMINATED | 1 RUNNING | 15 PENDING\n","Current time: 2023-10-17 12:04:44. Total running time: 22min 3s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00009   RUNNING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","4 more TERMINATED, 10 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=8024)\u001b[0m Starting distributed worker processes: ['8080 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00009_9_batch_size=16,learning_rate=0.0000,warmup_steps=134_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4552.75 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4262.58 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4540.75 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4462.79 examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4689.71 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4716.51 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4540.75 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m 2023-10-17 12:05:05.385722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 9 TERMINATED | 1 RUNNING | 15 PENDING\n","Current time: 2023-10-17 12:05:14. Total running time: 22min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00009   RUNNING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","4 more TERMINATED, 10 more PENDING\n","Trial status: 9 TERMINATED | 1 RUNNING | 15 PENDING\n","Current time: 2023-10-17 12:05:44. Total running time: 23min 3s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00009   RUNNING                 1.70707e-06                       16                      134                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","4 more TERMINATED, 10 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00009_9_batch_size=16,learning_rate=0.0000,warmup_steps=134_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 9 TERMINATED | 1 RUNNING | 15 PENDING\n","Current time: 2023-10-17 12:06:14. Total running time: 23min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00009   RUNNING                 1.70707e-06                       16                      134        1            74.2685             0.386459         0.601548     0.602703     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","4 more TERMINATED, 10 more PENDING\n","Trial status: 9 TERMINATED | 1 RUNNING | 15 PENDING\n","Current time: 2023-10-17 12:06:44. Total running time: 24min 3s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00009   RUNNING                 1.70707e-06                       16                      134        1            74.2685             0.386459         0.601548     0.602703     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00010   PENDING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","4 more TERMINATED, 10 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=8080)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00009_9_batch_size=16,learning_rate=0.0000,warmup_steps=134_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00009 completed after 2 iterations at 2023-10-17 12:06:52. Total running time: 24min 10s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00009 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000001 |\n","| time_this_iter_s                                   54.43804 |\n","| time_total_s                                      128.70653 |\n","| training_iteration                                        2 |\n","| accuracy                                            0.71078 |\n","| epoch                                                     1 |\n","| f1                                                  0.82388 |\n","| step                                                    460 |\n","| train/train_average_loss                            0.68397 |\n","| train/train_loss                                    0.38617 |\n","| val/val_average_loss                                0.55936 |\n","| val/val_loss                                        0.55941 |\n","| val_loss                                            0.55936 |\n","+-------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00010 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00010 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        4 |\n","| train_loop_config/learning_rate           2.11474e-06 |\n","| train_loop_config/warmup_steps                     59 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=8689)\u001b[0m Starting distributed worker processes: ['8745 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00010_10_batch_size=4,learning_rate=0.0000,warmup_steps=59_2023-10-17_11-42-41/lightning_logs\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:07:14. Total running time: 24min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m \rMap:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4582.22 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4433.00 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 3378.96 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 3255.05 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2191.43 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2669.47 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2527.97 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m 2023-10-17 12:07:23.132351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:07:45. Total running time: 25min 3s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n","Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:08:15. Total running time: 25min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00010_10_batch_size=4,learning_rate=0.0000,warmup_steps=59_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:08:45. Total running time: 26min 3s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59        1            96.9411             0.233225         0.543631     0.543631     0.708333 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n","Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:09:15. Total running time: 26min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59        1            96.9411             0.233225         0.543631     0.543631     0.708333 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n","Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:09:45. Total running time: 27min 3s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59        1            96.9411             0.233225         0.543631     0.543631     0.708333 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00010_10_batch_size=4,learning_rate=0.0000,warmup_steps=59_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:10:15. Total running time: 27min 33s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59        2           172.25               0.232782         0.459362     0.459361     0.784314 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n","Trial status: 10 TERMINATED | 1 RUNNING | 14 PENDING\n","Current time: 2023-10-17 12:10:45. Total running time: 28min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00010   RUNNING                 2.11474e-06                        4                       59        2           172.25               0.232782         0.459362     0.459361     0.784314 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","5 more TERMINATED, 9 more PENDING\n","\n","Trial TorchTrainer_4753d_00010 completed after 3 iterations at 2023-10-17 12:11:12. Total running time: 28min 30s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00010 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000002 |\n","| time_this_iter_s                                   81.10297 |\n","| time_total_s                                      253.35329 |\n","| training_iteration                                        3 |\n","| accuracy                                            0.82843 |\n","| epoch                                                     2 |\n","| f1                                                  0.88176 |\n","| step                                                   2751 |\n","| train/train_average_loss                            0.51038 |\n","| train/train_loss                                    0.53653 |\n","| val/val_average_loss                                0.41402 |\n","| val/val_loss                                        0.41402 |\n","| val_loss                                            0.41402 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=8745)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00010_10_batch_size=4,learning_rate=0.0000,warmup_steps=59_2023-10-17_11-42-41/checkpoint_000002)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 11 TERMINATED | 14 PENDING\n","Current time: 2023-10-17 12:11:15. Total running time: 28min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00011   PENDING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 9 more PENDING\n","\n","Trial TorchTrainer_4753d_00011 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00011 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        8 |\n","| train_loop_config/learning_rate           8.45439e-05 |\n","| train_loop_config/warmup_steps                     52 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=9876)\u001b[0m Starting distributed worker processes: ['9934 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warnings.warn(\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4345.79 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4495.06 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4414.57 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00011_11_batch_size=8,learning_rate=0.0001,warmup_steps=52_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2145.01 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2230.45 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2285.38 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2374.12 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2410.13 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2779.15 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2598.68 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m 2023-10-17 12:11:44.615910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 11 TERMINATED | 1 RUNNING | 13 PENDING\n","Current time: 2023-10-17 12:11:45. Total running time: 29min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00011   RUNNING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 8 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 11 TERMINATED | 1 RUNNING | 13 PENDING\n","Current time: 2023-10-17 12:12:15. Total running time: 29min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00011   RUNNING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 8 more PENDING\n","Trial status: 11 TERMINATED | 1 RUNNING | 13 PENDING\n","Current time: 2023-10-17 12:12:45. Total running time: 30min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00011   RUNNING                 8.45439e-05                        8                       52                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 8 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00011_11_batch_size=8,learning_rate=0.0001,warmup_steps=52_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 11 TERMINATED | 1 RUNNING | 13 PENDING\n","Current time: 2023-10-17 12:13:15. Total running time: 30min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00011   RUNNING                 8.45439e-05                        8                       52        1            87.9529             0.126574         0.548955     0.548955     0.730392 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 8 more PENDING\n","Trial status: 11 TERMINATED | 1 RUNNING | 13 PENDING\n","Current time: 2023-10-17 12:13:45. Total running time: 31min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00011   RUNNING                 8.45439e-05                        8                       52        1            87.9529             0.126574         0.548955     0.548955     0.730392 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 8 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00011_11_batch_size=8,learning_rate=0.0001,warmup_steps=52_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 11 TERMINATED | 1 RUNNING | 13 PENDING\n","Current time: 2023-10-17 12:14:15. Total running time: 31min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00011   RUNNING                 8.45439e-05                        8                       52        2           156.408              0.161516         0.442943     0.442943     0.786765 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 8 more PENDING\n","Trial status: 11 TERMINATED | 1 RUNNING | 13 PENDING\n","Current time: 2023-10-17 12:14:45. Total running time: 32min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00011   RUNNING                 8.45439e-05                        8                       52        2           156.408              0.161516         0.442943     0.442943     0.786765 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00012   PENDING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","6 more TERMINATED, 8 more PENDING\n","\n","Trial TorchTrainer_4753d_00011 completed after 3 iterations at 2023-10-17 12:14:56. Total running time: 32min 15s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00011 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000002 |\n","| time_this_iter_s                                   61.10897 |\n","| time_total_s                                      217.51651 |\n","| training_iteration                                        3 |\n","| accuracy                                             0.7402 |\n","| epoch                                                     2 |\n","| f1                                                      0.8 |\n","| step                                                   1377 |\n","| train/train_average_loss                             0.3907 |\n","| train/train_loss                                    0.09955 |\n","| val/val_average_loss                                0.58062 |\n","| val/val_loss                                        0.58062 |\n","| val_loss                                            0.58062 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=9934)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00011_11_batch_size=8,learning_rate=0.0001,warmup_steps=52_2023-10-17_11-42-41/checkpoint_000002)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00012 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00012 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                       16 |\n","| train_loop_config/learning_rate           1.60372e-05 |\n","| train_loop_config/warmup_steps                    171 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=10913)\u001b[0m Starting distributed worker processes: ['10973 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 12 TERMINATED | 1 RUNNING | 12 PENDING\n","Current time: 2023-10-17 12:15:15. Total running time: 32min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00012   RUNNING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","7 more TERMINATED, 7 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00012_12_batch_size=16,learning_rate=0.0000,warmup_steps=171_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2205.07 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2555.00 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2518.00 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2473.49 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2342.07 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4689.69 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4521.94 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m 2023-10-17 12:15:29.248805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 12 TERMINATED | 1 RUNNING | 12 PENDING\n","Current time: 2023-10-17 12:15:46. Total running time: 33min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00012   RUNNING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","7 more TERMINATED, 7 more PENDING\n","Trial status: 12 TERMINATED | 1 RUNNING | 12 PENDING\n","Current time: 2023-10-17 12:16:16. Total running time: 33min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00012   RUNNING                 1.60372e-05                       16                      171                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","7 more TERMINATED, 7 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00012_12_batch_size=16,learning_rate=0.0000,warmup_steps=171_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 12 TERMINATED | 1 RUNNING | 12 PENDING\n","Current time: 2023-10-17 12:16:46. Total running time: 34min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00002 with val_loss=0.4060378968715668 and params={'train_loop_config': {'learning_rate': 1.9517224641449515e-06, 'batch_size': 4, 'warmup_steps': 99}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00012   RUNNING                 1.60372e-05                       16                      171        1            76.0617             0.184457         0.513457     0.511172     0.754902 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","7 more TERMINATED, 7 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00012_12_batch_size=16,learning_rate=0.0000,warmup_steps=171_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 12 TERMINATED | 1 RUNNING | 12 PENDING\n","Current time: 2023-10-17 12:17:16. Total running time: 34min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.35121408104896545 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00012   RUNNING                 1.60372e-05                       16                      171        2           130.519              0.451586         0.352144     0.351214     0.855392 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","7 more TERMINATED, 7 more PENDING\n","Trial status: 12 TERMINATED | 1 RUNNING | 12 PENDING\n","Current time: 2023-10-17 12:17:46. Total running time: 35min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.35121408104896545 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00012   RUNNING                 1.60372e-05                       16                      171        2           130.519              0.451586         0.352144     0.351214     0.855392 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","7 more TERMINATED, 7 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=10973)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00012_12_batch_size=16,learning_rate=0.0000,warmup_steps=171_2023-10-17_11-42-41/checkpoint_000002)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00012 completed after 3 iterations at 2023-10-17 12:18:10. Total running time: 35min 29s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00012 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000002 |\n","| time_this_iter_s                                   54.61322 |\n","| time_total_s                                       185.1326 |\n","| training_iteration                                        3 |\n","| accuracy                                            0.85294 |\n","| epoch                                                     2 |\n","| f1                                                  0.89691 |\n","| step                                                    690 |\n","| train/train_average_loss                            0.41602 |\n","| train/train_loss                                     0.2791 |\n","| val/val_average_loss                                0.36499 |\n","| val/val_loss                                        0.36472 |\n","| val_loss                                            0.36499 |\n","+-------------------------------------------------------------+\n","\n","Trial status: 13 TERMINATED | 12 PENDING\n","Current time: 2023-10-17 12:18:16. Total running time: 35min 34s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00013   PENDING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","8 more TERMINATED, 7 more PENDING\n","\n","Trial TorchTrainer_4753d_00013 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00013 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        8 |\n","| train_loop_config/learning_rate           1.48574e-06 |\n","| train_loop_config/warmup_steps                     80 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=11814)\u001b[0m Starting distributed worker processes: ['11874 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00013_13_batch_size=8,learning_rate=0.0000,warmup_steps=80_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4497.06 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4496.95 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4459.54 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4478.87 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4808.45 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4919.74 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4605.80 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m 2023-10-17 12:18:40.193959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 13 TERMINATED | 1 RUNNING | 11 PENDING\n","Current time: 2023-10-17 12:18:46. Total running time: 36min 4s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00013   RUNNING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","8 more TERMINATED, 6 more PENDING\n","Trial status: 13 TERMINATED | 1 RUNNING | 11 PENDING\n","Current time: 2023-10-17 12:19:16. Total running time: 36min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00013   RUNNING                 1.48574e-06                        8                       80                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","8 more TERMINATED, 6 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00013_13_batch_size=8,learning_rate=0.0000,warmup_steps=80_2023-10-17_11-42-41/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 13 TERMINATED | 1 RUNNING | 11 PENDING\n","Current time: 2023-10-17 12:19:46. Total running time: 37min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00013   RUNNING                 1.48574e-06                        8                       80        1            80.3533             0.317352         0.596748     0.596748     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","8 more TERMINATED, 6 more PENDING\n","Trial status: 13 TERMINATED | 1 RUNNING | 11 PENDING\n","Current time: 2023-10-17 12:20:16. Total running time: 37min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00013   RUNNING                 1.48574e-06                        8                       80        1            80.3533             0.317352         0.596748     0.596748     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","8 more TERMINATED, 6 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=11874)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00013_13_batch_size=8,learning_rate=0.0000,warmup_steps=80_2023-10-17_11-42-41/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00013 completed after 2 iterations at 2023-10-17 12:20:43. Total running time: 38min 2s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00013 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000001 |\n","| time_this_iter_s                                   64.47971 |\n","| time_total_s                                      144.83306 |\n","| training_iteration                                        2 |\n","| accuracy                                            0.70833 |\n","| epoch                                                     1 |\n","| f1                                                  0.82265 |\n","| step                                                    918 |\n","| train/train_average_loss                            0.63339 |\n","| train/train_loss                                    0.41197 |\n","| val/val_average_loss                                0.55056 |\n","| val/val_loss                                        0.55056 |\n","| val_loss                                            0.55056 |\n","+-------------------------------------------------------------+\n","\n","Trial status: 14 TERMINATED | 11 PENDING\n","Current time: 2023-10-17 12:20:46. Total running time: 38min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00014   PENDING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","9 more TERMINATED, 6 more PENDING\n","\n","Trial TorchTrainer_4753d_00014 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00014 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                       8 |\n","| train_loop_config/learning_rate           0.00205405 |\n","| train_loop_config/warmup_steps                   133 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=12549)\u001b[0m Starting distributed worker processes: ['12603 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00014_14_batch_size=8,learning_rate=0.0021,warmup_steps=133_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2176.54 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2335.31 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2442.73 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2371.92 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2968.79 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2084.00 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2058.67 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2029.08 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m 2023-10-17 12:21:16.286463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 14 TERMINATED | 1 RUNNING | 10 PENDING\n","Current time: 2023-10-17 12:21:16. Total running time: 38min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00014   RUNNING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","9 more TERMINATED, 5 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 14 TERMINATED | 1 RUNNING | 10 PENDING\n","Current time: 2023-10-17 12:21:46. Total running time: 39min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00014   RUNNING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","9 more TERMINATED, 5 more PENDING\n","Trial status: 14 TERMINATED | 1 RUNNING | 10 PENDING\n","Current time: 2023-10-17 12:22:16. Total running time: 39min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00014   RUNNING                 0.00205405                         8                      133                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00015   PENDING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","9 more TERMINATED, 5 more PENDING\n","\n","Trial TorchTrainer_4753d_00014 completed after 1 iterations at 2023-10-17 12:22:27. Total running time: 39min 46s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00014 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   97.51449 |\n","| time_total_s                                       97.51449 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    459 |\n","| train/train_loss                                    0.31842 |\n","| val/val_average_loss                                0.63374 |\n","| val/val_loss                                        0.63374 |\n","| val_loss                                            0.63374 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=12603)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00014_14_batch_size=8,learning_rate=0.0021,warmup_steps=133_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00015 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00015 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        4 |\n","| train_loop_config/learning_rate           0.000689448 |\n","| train_loop_config/warmup_steps                    190 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=13072)\u001b[0m Starting distributed worker processes: ['13136 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 15 TERMINATED | 1 RUNNING | 9 PENDING\n","Current time: 2023-10-17 12:22:46. Total running time: 40min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00015   RUNNING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","10 more TERMINATED, 4 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00015_15_batch_size=4,learning_rate=0.0007,warmup_steps=190_2023-10-17_11-42-41/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2256.82 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2430.29 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2311.52 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2355.83 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2573.46 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4873.13 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4705.05 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m 2023-10-17 12:23:00.429859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 15 TERMINATED | 1 RUNNING | 9 PENDING\n","Current time: 2023-10-17 12:23:16. Total running time: 40min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00015   RUNNING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","10 more TERMINATED, 4 more PENDING\n","Trial status: 15 TERMINATED | 1 RUNNING | 9 PENDING\n","Current time: 2023-10-17 12:23:47. Total running time: 41min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00015   RUNNING                 0.000689448                        4                      190                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","10 more TERMINATED, 4 more PENDING\n","\n","Trial TorchTrainer_4753d_00015 completed after 1 iterations at 2023-10-17 12:24:12. Total running time: 41min 30s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00015 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   96.49626 |\n","| time_total_s                                       96.49626 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    917 |\n","| train/train_loss                                    0.34026 |\n","| val/val_average_loss                                0.63339 |\n","| val/val_loss                                        0.63339 |\n","| val_loss                                            0.63339 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=13136)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00015_15_batch_size=4,learning_rate=0.0007,warmup_steps=190_2023-10-17_11-42-41/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 16 TERMINATED | 9 PENDING\n","Current time: 2023-10-17 12:24:17. Total running time: 41min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00016   PENDING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more TERMINATED, 4 more PENDING\n","\n","Trial TorchTrainer_4753d_00016 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00016 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        8 |\n","| train_loop_config/learning_rate           0.000177325 |\n","| train_loop_config/warmup_steps                    189 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=13597)\u001b[0m Starting distributed worker processes: ['13651 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00016_16_batch_size=8,learning_rate=0.0002,warmup_steps=189_2023-10-17_11-42-58/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4412.14 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4400.91 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4417.24 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4405.83 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4528.87 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4948.58 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4587.90 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m 2023-10-17 12:24:40.948735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 16 TERMINATED | 1 RUNNING | 8 PENDING\n","Current time: 2023-10-17 12:24:47. Total running time: 42min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00016   RUNNING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more TERMINATED, 3 more PENDING\n","Trial status: 16 TERMINATED | 1 RUNNING | 8 PENDING\n","Current time: 2023-10-17 12:25:17. Total running time: 42min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00016   RUNNING                 0.000177325                        8                      189                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","11 more TERMINATED, 3 more PENDING\n","\n","Trial TorchTrainer_4753d_00016 completed after 1 iterations at 2023-10-17 12:25:45. Total running time: 43min 4s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00016 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   87.18712 |\n","| time_total_s                                       87.18712 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    459 |\n","| train/train_loss                                    0.33656 |\n","| val/val_average_loss                                0.63029 |\n","| val/val_loss                                        0.63029 |\n","| val_loss                                            0.63029 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=13651)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00016_16_batch_size=8,learning_rate=0.0002,warmup_steps=189_2023-10-17_11-42-58/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 17 TERMINATED | 8 PENDING\n","Current time: 2023-10-17 12:25:47. Total running time: 43min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00017   PENDING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","12 more TERMINATED, 3 more PENDING\n","\n","Trial TorchTrainer_4753d_00017 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00017 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                      16 |\n","| train_loop_config/learning_rate           0.00432872 |\n","| train_loop_config/warmup_steps                   199 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=14076)\u001b[0m Starting distributed worker processes: ['14130 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00017_17_batch_size=16,learning_rate=0.0043,warmup_steps=199_2023-10-17_11-47-35/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4443.74 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4535.79 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4513.75 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4470.97 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4335.91 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4998.12 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4663.91 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m 2023-10-17 12:26:14.918831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 17 TERMINATED | 1 RUNNING | 7 PENDING\n","Current time: 2023-10-17 12:26:17. Total running time: 43min 35s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00017   RUNNING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","12 more TERMINATED, 2 more PENDING\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 17 TERMINATED | 1 RUNNING | 7 PENDING\n","Current time: 2023-10-17 12:26:47. Total running time: 44min 5s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00017   RUNNING                 0.00432872                        16                      199                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","12 more TERMINATED, 2 more PENDING\n","\n","Trial TorchTrainer_4753d_00017 completed after 1 iterations at 2023-10-17 12:27:10. Total running time: 44min 29s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00017 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                    78.4893 |\n","| time_total_s                                        78.4893 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    230 |\n","| train/train_loss                                    0.34173 |\n","| val/val_average_loss                                0.64564 |\n","| val/val_loss                                        0.64426 |\n","| val_loss                                            0.64564 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=14130)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00017_17_batch_size=16,learning_rate=0.0043,warmup_steps=199_2023-10-17_11-47-35/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 18 TERMINATED | 7 PENDING\n","Current time: 2023-10-17 12:27:17. Total running time: 44min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00018   PENDING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","13 more TERMINATED, 2 more PENDING\n","\n","Trial TorchTrainer_4753d_00018 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00018 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        8 |\n","| train_loop_config/learning_rate           0.000401949 |\n","| train_loop_config/warmup_steps                     39 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=14521)\u001b[0m Starting distributed worker processes: ['14585 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00018_18_batch_size=8,learning_rate=0.0004,warmup_steps=39_2023-10-17_11-49-04/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2517.26 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 3487.67 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 3892.73 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 3735.34 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4091.70 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4902.18 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4622.15 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m 2023-10-17 12:27:42.169748: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 18 TERMINATED | 1 RUNNING | 6 PENDING\n","Current time: 2023-10-17 12:27:47. Total running time: 45min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00018   RUNNING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","13 more TERMINATED, 1 more PENDING\n","Trial status: 18 TERMINATED | 1 RUNNING | 6 PENDING\n","Current time: 2023-10-17 12:28:17. Total running time: 45min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00018   RUNNING                 0.000401949                        8                       39                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","13 more TERMINATED, 1 more PENDING\n","\n","Trial TorchTrainer_4753d_00018 completed after 1 iterations at 2023-10-17 12:28:40. Total running time: 45min 58s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00018 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   80.25797 |\n","| time_total_s                                       80.25797 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    459 |\n","| train/train_loss                                    0.34204 |\n","| val/val_average_loss                                0.63291 |\n","| val/val_loss                                        0.63291 |\n","| val_loss                                            0.63291 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=14585)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00018_18_batch_size=8,learning_rate=0.0004,warmup_steps=39_2023-10-17_11-49-04/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 19 TERMINATED | 6 PENDING\n","Current time: 2023-10-17 12:28:47. Total running time: 46min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00019   PENDING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","14 more TERMINATED, 1 more PENDING\n","\n","Trial TorchTrainer_4753d_00019 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00019 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                      16 |\n","| train_loop_config/learning_rate           0.00545256 |\n","| train_loop_config/warmup_steps                    52 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=14984)\u001b[0m Starting distributed worker processes: ['15042 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00019_19_batch_size=16,learning_rate=0.0055,warmup_steps=52_2023-10-17_11-53-43/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2419.50 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2656.32 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2558.24 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2560.22 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2751.79 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4770.16 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4650.71 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m 2023-10-17 12:29:11.956374: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 19 TERMINATED | 1 RUNNING | 5 PENDING\n","Current time: 2023-10-17 12:29:17. Total running time: 46min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00019   RUNNING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","14 more TERMINATED\n","Trial status: 19 TERMINATED | 1 RUNNING | 5 PENDING\n","Current time: 2023-10-17 12:29:47. Total running time: 47min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00019   RUNNING                 0.00545256                        16                       52                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00020   PENDING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","14 more TERMINATED\n","\n","Trial TorchTrainer_4753d_00019 completed after 1 iterations at 2023-10-17 12:30:03. Total running time: 47min 22s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00019 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   75.14059 |\n","| time_total_s                                       75.14059 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    230 |\n","| train/train_loss                                    0.44239 |\n","| val/val_average_loss                                0.62499 |\n","| val/val_loss                                        0.62416 |\n","| val_loss                                            0.62499 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=15042)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00019_19_batch_size=16,learning_rate=0.0055,warmup_steps=52_2023-10-17_11-53-43/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00020 started with configuration:\n","+-----------------------------------------------------+\n","| Trial TorchTrainer_4753d_00020 config               |\n","+-----------------------------------------------------+\n","| train_loop_config/batch_size                      4 |\n","| train_loop_config/learning_rate           0.0670311 |\n","| train_loop_config/warmup_steps                   40 |\n","+-----------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=15417)\u001b[0m Starting distributed worker processes: ['15475 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 20 TERMINATED | 1 RUNNING | 4 PENDING\n","Current time: 2023-10-17 12:30:17. Total running time: 47min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00020   RUNNING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","15 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00020_20_batch_size=4,learning_rate=0.0670,warmup_steps=40_2023-10-17_11-55-25/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4385.84 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4706.97 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4655.72 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4550.47 examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 4859.50 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4925.86 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4718.33 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m 2023-10-17 12:30:32.862536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 20 TERMINATED | 1 RUNNING | 4 PENDING\n","Current time: 2023-10-17 12:30:47. Total running time: 48min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00020   RUNNING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","15 more TERMINATED\n","Trial status: 20 TERMINATED | 1 RUNNING | 4 PENDING\n","Current time: 2023-10-17 12:31:17. Total running time: 48min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00020   RUNNING                 0.0670311                          4                       40                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","15 more TERMINATED\n","\n","Trial TorchTrainer_4753d_00020 completed after 1 iterations at 2023-10-17 12:31:43. Total running time: 49min 1s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00020 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   92.64154 |\n","| time_total_s                                       92.64154 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    917 |\n","| train/train_loss                                    0.30087 |\n","| val/val_average_loss                                0.63441 |\n","| val/val_loss                                        0.63441 |\n","| val_loss                                            0.63441 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=15475)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00020_20_batch_size=4,learning_rate=0.0670,warmup_steps=40_2023-10-17_11-55-25/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 21 TERMINATED | 4 PENDING\n","Current time: 2023-10-17 12:31:47. Total running time: 49min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00021   PENDING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n","\n","Trial TorchTrainer_4753d_00021 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00021 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                        4 |\n","| train_loop_config/learning_rate           6.70401e-06 |\n","| train_loop_config/warmup_steps                     70 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=15924)\u001b[0m Starting distributed worker processes: ['15976 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00021_21_batch_size=4,learning_rate=0.0000,warmup_steps=70_2023-10-17_11-56-58/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4474.28 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4488.89 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 3249.49 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 3263.11 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2118.09 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2499.00 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2459.62 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m 2023-10-17 12:32:15.420296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:32:17. Total running time: 49min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:32:48. Total running time: 50min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n","Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:33:18. Total running time: 50min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00021_21_batch_size=4,learning_rate=0.0000,warmup_steps=70_2023-10-17_11-56-58/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:33:48. Total running time: 51min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70        1           106.72               0.12267          0.443486     0.443486     0.79902  |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n","Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:34:18. Total running time: 51min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70        1           106.72               0.12267          0.443486     0.443486     0.79902  |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n","Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:34:48. Total running time: 52min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00012 with val_loss=0.3649887442588806 and params={'train_loop_config': {'learning_rate': 1.6037155126499187e-05, 'batch_size': 16, 'warmup_steps': 171}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70        1           106.72               0.12267          0.443486     0.443486     0.79902  |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00021_21_batch_size=4,learning_rate=0.0000,warmup_steps=70_2023-10-17_11-56-58/checkpoint_000001)\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:35:18. Total running time: 52min 36s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.35177135467529297 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70        2           191.965              0.373974         0.351771     0.351771     0.867647 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n","Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:35:48. Total running time: 53min 6s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.35177135467529297 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70        2           191.965              0.373974         0.351771     0.351771     0.867647 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n","Trial status: 21 TERMINATED | 1 RUNNING | 3 PENDING\n","Current time: 2023-10-17 12:36:18. Total running time: 53min 37s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.35177135467529297 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00021   RUNNING                 6.70401e-06                        4                       70        2           191.965              0.373974         0.351771     0.351771     0.867647 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00022   PENDING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","16 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=15976)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00021_21_batch_size=4,learning_rate=0.0000,warmup_steps=70_2023-10-17_11-56-58/checkpoint_000002)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00021 completed after 3 iterations at 2023-10-17 12:36:22. Total running time: 53min 40s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00021 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000002 |\n","| time_this_iter_s                                   80.69672 |\n","| time_total_s                                      272.66211 |\n","| training_iteration                                        3 |\n","| accuracy                                            0.86029 |\n","| epoch                                                     2 |\n","| f1                                                  0.90121 |\n","| step                                                   2751 |\n","| train/train_average_loss                            0.36367 |\n","| train/train_loss                                    0.09458 |\n","| val/val_average_loss                                0.35308 |\n","| val/val_loss                                        0.35308 |\n","| val_loss                                            0.35308 |\n","+-------------------------------------------------------------+\n","\n","Trial TorchTrainer_4753d_00022 started with configuration:\n","+-------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00022 config                 |\n","+-------------------------------------------------------+\n","| train_loop_config/batch_size                       16 |\n","| train_loop_config/learning_rate           9.42774e-05 |\n","| train_loop_config/warmup_steps                    138 |\n","+-------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=17187)\u001b[0m Starting distributed worker processes: ['17247 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00022_22_batch_size=16,learning_rate=0.0001,warmup_steps=138_2023-10-17_11-58-28/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2291.34 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2286.95 examples/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 22 TERMINATED | 1 RUNNING | 2 PENDING\n","Current time: 2023-10-17 12:36:48. Total running time: 54min 7s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00022   RUNNING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","17 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2411.06 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2342.70 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2464.25 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2545.88 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2404.97 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m 2023-10-17 12:36:55.405366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 22 TERMINATED | 1 RUNNING | 2 PENDING\n","Current time: 2023-10-17 12:37:18. Total running time: 54min 37s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00022   RUNNING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","17 more TERMINATED\n","Trial status: 22 TERMINATED | 1 RUNNING | 2 PENDING\n","Current time: 2023-10-17 12:37:48. Total running time: 55min 7s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00022   RUNNING                 9.42774e-05                       16                      138                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00023   PENDING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","17 more TERMINATED\n","\n","Trial TorchTrainer_4753d_00022 completed after 1 iterations at 2023-10-17 12:37:56. Total running time: 55min 15s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00022 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   87.11867 |\n","| time_total_s                                       87.11867 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.72794 |\n","| epoch                                                     0 |\n","| f1                                                   0.8224 |\n","| step                                                    230 |\n","| train/train_loss                                    0.19568 |\n","| val/val_average_loss                                0.63363 |\n","| val/val_loss                                        0.63936 |\n","| val_loss                                            0.63363 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=17247)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00022_22_batch_size=16,learning_rate=0.0001,warmup_steps=138_2023-10-17_11-58-28/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00023 started with configuration:\n","+------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00023 config                |\n","+------------------------------------------------------+\n","| train_loop_config/batch_size                      16 |\n","| train_loop_config/learning_rate           0.00360338 |\n","| train_loop_config/warmup_steps                    32 |\n","+------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=17670)\u001b[0m Starting distributed worker processes: ['17736 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00023_23_batch_size=16,learning_rate=0.0036,warmup_steps=32_2023-10-17_12-00-51/lightning_logs\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 23 TERMINATED | 1 RUNNING | 1 PENDING\n","Current time: 2023-10-17 12:38:18. Total running time: 55min 37s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00023   RUNNING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","18 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m \rMap:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:01, 2488.64 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 2498.36 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:01<00:00, 2660.15 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:01<00:00, 2568.83 examples/s]\n","Map:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 2301.55 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 4448.17 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 4464.13 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m 2023-10-17 12:38:29.261689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 23 TERMINATED | 1 RUNNING | 1 PENDING\n","Current time: 2023-10-17 12:38:49. Total running time: 56min 7s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00023   RUNNING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","18 more TERMINATED\n","Trial status: 23 TERMINATED | 1 RUNNING | 1 PENDING\n","Current time: 2023-10-17 12:39:19. Total running time: 56min 37s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00023   RUNNING                 0.00360338                        16                       32                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00024   PENDING                 0.042806                           8                       40                                                                                             |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","18 more TERMINATED\n","\n","Trial TorchTrainer_4753d_00023 completed after 1 iterations at 2023-10-17 12:39:22. Total running time: 56min 41s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00023 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000000 |\n","| time_this_iter_s                                   77.95904 |\n","| time_total_s                                       77.95904 |\n","| training_iteration                                        1 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     0 |\n","| f1                                                  0.81223 |\n","| step                                                    230 |\n","| train/train_loss                                    0.37683 |\n","| val/val_average_loss                                0.63018 |\n","| val/val_loss                                        0.62905 |\n","| val_loss                                            0.63018 |\n","+-------------------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=17736)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00023_23_batch_size=16,learning_rate=0.0036,warmup_steps=32_2023-10-17_12-00-51/checkpoint_000000)\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial TorchTrainer_4753d_00024 started with configuration:\n","+----------------------------------------------------+\n","| Trial TorchTrainer_4753d_00024 config              |\n","+----------------------------------------------------+\n","| train_loop_config/batch_size                     8 |\n","| train_loop_config/learning_rate           0.042806 |\n","| train_loop_config/warmup_steps                  40 |\n","+----------------------------------------------------+\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(TorchTrainer pid=18122)\u001b[0m Starting distributed worker processes: ['18176 (172.28.0.12)']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m <ipython-input-4-3fef8ccecca7>:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m GPU available: True (cuda), used: True\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m TPU available: False, using: 0 TPU cores\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m IPU available: False, using: 0 IPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m HPU available: False, using: 0 HPUs\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   rank_zero_warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m [rank: 0] Global seed set to 42\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m Missing logger folder: /root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00024_24_batch_size=8,learning_rate=0.0428,warmup_steps=40_2023-10-17_12-03-15/lightning_logs\n","Map:   0%|          | 0/3668 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   warnings.warn(\n","Map:  27%|██▋       | 1000/3668 [00:00<00:00, 4402.96 examples/s]\n","Map:  55%|█████▍    | 2000/3668 [00:00<00:00, 4305.90 examples/s]\n","Map:  82%|████████▏ | 3000/3668 [00:00<00:00, 4367.39 examples/s]\n","Map: 100%|██████████| 3668/3668 [00:00<00:00, 4397.33 examples/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Trial status: 24 TERMINATED | 1 RUNNING\n","Current time: 2023-10-17 12:39:49. Total running time: 57min 7s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00024   RUNNING                 0.042806                           8                       40                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","19 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m \rMap:   0%|          | 0/408 [00:00<?, ? examples/s]\n","Map: 100%|██████████| 408/408 [00:00<00:00, 3666.90 examples/s]\n","Map:   0%|          | 0/1725 [00:00<?, ? examples/s]\n","Map:  58%|█████▊    | 1000/1725 [00:00<00:00, 2612.20 examples/s]\n","Map: 100%|██████████| 1725/1725 [00:00<00:00, 2486.76 examples/s]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m \n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   | Name  | Type                                | Params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m 0 | model | DistilBertForSequenceClassification | 67.0 M\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m --------------------------------------------------------------\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m 67.0 M    Trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m 0         Non-trainable params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m 67.0 M    Total params\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m 267.820   Total estimated model params size (MB)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m 2023-10-17 12:39:53.521825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('f1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('val/val_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   warning_cache.warn(\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m [W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 24 TERMINATED | 1 RUNNING\n","Current time: 2023-10-17 12:40:19. Total running time: 57min 37s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00024   RUNNING                 0.042806                           8                       40                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","19 more TERMINATED\n","Trial status: 24 TERMINATED | 1 RUNNING\n","Current time: 2023-10-17 12:40:49. Total running time: 58min 8s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00024   RUNNING                 0.042806                           8                       40                                                                                             |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","19 more TERMINATED\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00024_24_batch_size=8,learning_rate=0.0428,warmup_steps=40_2023-10-17_12-03-15/checkpoint_000000)\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:539: PossibleUserWarning: It is recommended to use `self.log('train/train_average_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n","\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m   warning_cache.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Trial status: 24 TERMINATED | 1 RUNNING\n","Current time: 2023-10-17 12:41:19. Total running time: 58min 38s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00024   RUNNING                 0.042806                           8                       40        1            88.7275             0.370353         0.624408     0.624408     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","19 more TERMINATED\n","Trial status: 24 TERMINATED | 1 RUNNING\n","Current time: 2023-10-17 12:41:49. Total running time: 59min 8s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00024   RUNNING                 0.042806                           8                       40        1            88.7275             0.370353         0.624408     0.624408     0.683824 |\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276              0.184073         0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277             0.380006         0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947              0.526327         0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639             0.364895         0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008             0.365824         0.624739     0.624739     0.683824 |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","19 more TERMINATED\n","\n","Trial TorchTrainer_4753d_00024 completed after 2 iterations at 2023-10-17 12:42:12. Total running time: 59min 30s\n","+-------------------------------------------------------------+\n","| Trial TorchTrainer_4753d_00024 result                       |\n","+-------------------------------------------------------------+\n","| checkpoint_dir_name                       checkpoint_000001 |\n","| time_this_iter_s                                   73.73367 |\n","| time_total_s                                      162.46114 |\n","| training_iteration                                        2 |\n","| accuracy                                            0.68382 |\n","| epoch                                                     1 |\n","| f1                                                  0.81223 |\n","| step                                                    918 |\n","| train/train_average_loss                            0.76651 |\n","| train/train_loss                                    0.76193 |\n","| val/val_average_loss                                0.62403 |\n","| val/val_loss                                        0.62403 |\n","| val_loss                                            0.62403 |\n","+-------------------------------------------------------------+\n","\n","Trial status: 25 TERMINATED\n","Current time: 2023-10-17 12:42:12. Total running time: 59min 30s\n","Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:None)\n","Current best trial: 4753d_00021 with val_loss=0.3530758321285248 and params={'train_loop_config': {'learning_rate': 6.704010646472272e-06, 'batch_size': 4, 'warmup_steps': 70}}\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| Trial name                 status         ...fig/learning_rate     ...config/batch_size     ...nfig/warmup_steps     iter     total time (s)     train/train_loss     val/val_loss     val_loss     accuracy |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","| TorchTrainer_4753d_00000   TERMINATED              7.45934e-05                        4                       14        3           268.276             0.184073          0.541319     0.541319     0.727941 |\n","| TorchTrainer_4753d_00001   TERMINATED              0.000984674                       16                      121        1            80.7277            0.380006          0.626601     0.627653     0.683824 |\n","| TorchTrainer_4753d_00002   TERMINATED              1.95172e-06                        4                       99        3           268.947             0.526327          0.406038     0.406038     0.828431 |\n","| TorchTrainer_4753d_00003   TERMINATED              0.00179656                         4                        1        1            94.8639            0.364895          0.637626     0.637625     0.683824 |\n","| TorchTrainer_4753d_00004   TERMINATED              0.0492905                          8                      191        1            85.7008            0.365824          0.624739     0.624739     0.683824 |\n","| TorchTrainer_4753d_00005   TERMINATED              0.00122295                         8                       21        1            84.0109            0.292249          0.644921     0.644921     0.683824 |\n","| TorchTrainer_4753d_00006   TERMINATED              1.3041e-06                        16                       58        2           133.142             0.519745          0.583399     0.58355      0.688725 |\n","| TorchTrainer_4753d_00007   TERMINATED              1.71131e-06                       16                      189        2           137.826             0.458952          0.565442     0.564998     0.705882 |\n","| TorchTrainer_4753d_00008   TERMINATED              0.00123575                        16                      243        1            81.42              0.337605          0.643182     0.644546     0.683824 |\n","| TorchTrainer_4753d_00009   TERMINATED              1.70707e-06                       16                      134        2           128.707             0.386166          0.559406     0.55936      0.710784 |\n","| TorchTrainer_4753d_00010   TERMINATED              2.11474e-06                        4                       59        3           253.353             0.536532          0.414023     0.414023     0.828431 |\n","| TorchTrainer_4753d_00011   TERMINATED              8.45439e-05                        8                       52        3           217.517             0.0995537         0.580625     0.580625     0.740196 |\n","| TorchTrainer_4753d_00012   TERMINATED              1.60372e-05                       16                      171        3           185.133             0.279101          0.364716     0.364989     0.852941 |\n","| TorchTrainer_4753d_00013   TERMINATED              1.48574e-06                        8                       80        2           144.833             0.41197           0.550558     0.550558     0.708333 |\n","| TorchTrainer_4753d_00014   TERMINATED              0.00205405                         8                      133        1            97.5145            0.31842           0.633745     0.633744     0.683824 |\n","| TorchTrainer_4753d_00015   TERMINATED              0.000689448                        4                      190        1            96.4963            0.340257          0.633394     0.633394     0.683824 |\n","| TorchTrainer_4753d_00016   TERMINATED              0.000177325                        8                      189        1            87.1871            0.33656           0.630288     0.630288     0.683824 |\n","| TorchTrainer_4753d_00017   TERMINATED              0.00432872                        16                      199        1            78.4893            0.341727          0.644264     0.645642     0.683824 |\n","| TorchTrainer_4753d_00018   TERMINATED              0.000401949                        8                       39        1            80.258             0.342041          0.632908     0.632908     0.683824 |\n","| TorchTrainer_4753d_00019   TERMINATED              0.00545256                        16                       52        1            75.1406            0.442393          0.624162     0.624985     0.683824 |\n","| TorchTrainer_4753d_00020   TERMINATED              0.0670311                          4                       40        1            92.6415            0.300865          0.634411     0.634411     0.683824 |\n","| TorchTrainer_4753d_00021   TERMINATED              6.70401e-06                        4                       70        3           272.662             0.094576          0.353076     0.353076     0.860294 |\n","| TorchTrainer_4753d_00022   TERMINATED              9.42774e-05                       16                      138        1            87.1187            0.195681          0.639359     0.633629     0.727941 |\n","| TorchTrainer_4753d_00023   TERMINATED              0.00360338                        16                       32        1            77.959             0.376834          0.629055     0.630176     0.683824 |\n","| TorchTrainer_4753d_00024   TERMINATED              0.042806                           8                       40        2           162.461             0.761926          0.624032     0.624032     0.683824 |\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(RayTrainWorker pid=18176)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00024_24_batch_size=8,learning_rate=0.0428,warmup_steps=40_2023-10-17_12-03-15/checkpoint_000001)\n"]}],"source":["def tune_mnist_asha(num_samples=10):\n","    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n","\n","    tuner = tune.Tuner(\n","        ray_trainer,\n","        param_space={\"train_loop_config\": search_space},\n","        tune_config=tune.TuneConfig(\n","            metric=\"val_loss\",\n","            mode=\"min\",\n","            num_samples=num_samples,\n","            scheduler=scheduler,\n","        ),\n","    )\n","    # RAY_memory_monitor_refresh_ms=0\n","    return tuner.fit()\n","\n","results = tune_mnist_asha(num_samples=num_samples)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1697546727565,"user":{"displayName":"Jean-Luc Bittel","userId":"17792745637704580684"},"user_tz":-120},"id":"nPoKO1s6qjGP","outputId":"b556205f-baa2-4a8c-a465-e73aa1fea8f4"},"outputs":[{"data":{"text/plain":["Result(\n","  metrics={'train/train_loss': 0.09457603096961975, 'val/val_loss': 0.3530758023262024, 'val_loss': 0.3530758321285248, 'accuracy': 0.8602941036224365, 'f1': 0.9012131690979004, 'val/val_average_loss': 0.3530758321285248, 'train/train_average_loss': 0.36366596817970276, 'epoch': 2, 'step': 2751},\n","  path='/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00021_21_batch_size=4,learning_rate=0.0000,warmup_steps=70_2023-10-17_11-56-58',\n","  filesystem='local',\n","  checkpoint=Checkpoint(filesystem=local, path=/root/ray_results/TorchTrainer_2023-10-17_11-42-32/TorchTrainer_4753d_00021_21_batch_size=4,learning_rate=0.0000,warmup_steps=70_2023-10-17_11-56-58/checkpoint_000002)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["results.get_best_result(metric=\"val_loss\", mode=\"min\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1J9YCDwPIzzz_d_dqasjWHzr7_meK-3M7","timestamp":1697376566865}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"004bc5e0b68d4b3f9f2939648092bfee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01f243c6f207448999be9f108666d0a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01f7a14072944f508e18c4be851f8d05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"026b743ebaec4d5ebb1838b6485b01bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02e3487391a5467686ac798e17f0c7c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"069d2f12ddcf4645bbfb7f30078c847a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07bd9d9ac50e42dbb60fa8a91a0ba403":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07ee38b3498d42f69ede7fb3e8708118":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09fb4d73fc834b0a92359db3f9994ec3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aec5670730744fd9d3a18a361dd4ff7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b0655faf264472785c7041e08534654":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfc085b6deea454cad3f9d46ee6f429c","placeholder":"​","style":"IPY_MODEL_cfa3e95e3b3d4b679aa09d5cf13a5ec9","value":" 441k/? [00:00&lt;00:00, 8.08MB/s]"}},"0ed84888dc6842fe896e9a3a67288257":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fed062a5c9604a6c861a4ee89929d77c","placeholder":"​","style":"IPY_MODEL_2b383ba2b4d04598ae9e21cd086e963d","value":"Map: 100%"}},"0f56be71d76b47838f65871a83785a5f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10851459e221472296e92b69f1ffdeb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43c18ffb110441b7899f1e1e80bfc8d8","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a56d9d332dcd4c75944c1cfc3bf182a0","value":466062}},"121cdf2eb5fa41c8abc02bc64d8a15d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c093365d36304e018fbd305a3a66ba71","max":3668,"min":0,"orientation":"horizontal","style":"IPY_MODEL_598bb4d4baa1419e81c95cb9072bbab1","value":3668}},"13676e154a0245b7a42d660c2acefe74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14728ac7e9be416bbd557dddf904353b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14c56fb1941e43cdb8adc486817f07bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1578e896c7b84d04aef3aa6f5acbeeca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6189683c97e643f2830bf915a573b899","max":28682,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be6aee17c20640ed95586fe17013b631","value":28682}},"163975a641254219b917214ebe1841c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"166bb8fddc27442b9ccf1018a7a7715f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b0f7aacd5e64c9c8000a9f9466c5a0e","placeholder":"​","style":"IPY_MODEL_c966d0f0f9834198bd362bbb6c816131","value":"Downloading readme: 100%"}},"17a68c93c6e94f5b810d9f21673a7114":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1acff480844e4fb9a684263a340fcf48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1bdf7c6b193441b3a3d5e4b3af4b815d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1beeff4a30b744b193ae46d8d6b588a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cc2ff419bb5438fb2778b52e203052a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f445e7cfff44e9fbaa1459eff2ce121":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e44c7e8818d540fea169feff2edaf66a","IPY_MODEL_6e4bb9727ddb445e98e3827dbf4f7993","IPY_MODEL_ab0bd56fd10d4e6194871a1b59c78b70"],"layout":"IPY_MODEL_ab5725e298c04d67ad70fd3073a0c420"}},"21a59f4ada4840c1bc9268d234a8e160":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22ddde21e3fe47fe92e50c96052d60aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b680c5211fbb432d9a159462e5002d52","max":3668,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21a59f4ada4840c1bc9268d234a8e160","value":3668}},"24611bdc999648fc834d3868ccf629fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"255506ff5107439abd85b0678a12232b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ea9a141e33a4a26a86b46c564864556","placeholder":"​","style":"IPY_MODEL_4132989cb9d1483a96c475fdcd36c243","value":" 3668/3668 [00:00&lt;00:00, 4608.15 examples/s]"}},"25b1b1dd39854d39b12c850142c2785d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25cee06da5b74f14b137c12c1f7cb670":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3077d90b8764186a245e34fd2399b07","placeholder":"​","style":"IPY_MODEL_681b767394f9430899a0b1e215851b06","value":" 408/408 [00:00&lt;00:00, 1266.94 examples/s]"}},"25d2de16b5b049618597c5f40cb5970a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d99576503ba64499a86048f1428f2e77","IPY_MODEL_95475e1e578b42848b7b56196aaccb49","IPY_MODEL_4c79638e1c664e6ca7ada2d1faa034b0"],"layout":"IPY_MODEL_4cd0b778bd7c489c9ecb3d56a43af724"}},"2701b457023b4edf80ccc6a98c5ef08d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b383ba2b4d04598ae9e21cd086e963d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ccf61e37cf645feb2552df01f4ca269":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d7070ecd36a4968801c25d181a006b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_84517d98e2ad4d86ae40030bbef86b56","IPY_MODEL_4f7e3c6c9d2c45bfbee63d95335400f5","IPY_MODEL_4586527c328e4ae397bde1bfe8b9a59e"],"layout":"IPY_MODEL_32a72319303b4c738c179672fe15f8db"}},"3039526144e548caab131a25789b82b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdd7ef365a0543928fd04e141acfb49d","placeholder":"​","style":"IPY_MODEL_a865a8cc363147029008ad76c02b707b","value":" 408/408 [00:00&lt;00:00, 3130.76 examples/s]"}},"30aeb0512c30413eb8c34e81dd141029":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3133415bebd147c6a141ada14746312c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32a72319303b4c738c179672fe15f8db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32d70ddf17b54066821f648f90883ca5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33952ceb871a46b9bea551be8f9e10b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"380f9e66449841249a422397cfc51fab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3810b0e869bf416cbfa85ca91bd8ac40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5d216d7d167840568c2c66f4e44597f2","IPY_MODEL_d6f39e6f87894457b5afa446ec8aa8f1","IPY_MODEL_25cee06da5b74f14b137c12c1f7cb670"],"layout":"IPY_MODEL_ad83b367ed6b4069926e86c5150297ce"}},"39df911a0eac45c78e918d4b387920a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffb2baec7254432f996a45d63db60b57","placeholder":"​","style":"IPY_MODEL_bb78b3609c69474090087ee75cff3dce","value":"Downloading (…)lve/main/config.json: 100%"}},"3a7783fb4e33468ca2d92dd752983f0d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b0f7a1c720e495488fa395fa796474c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d502bf65b7c24600a0d4a9d85130b05e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_710f97a41f364d169cf7077cd6b5cc1e","value":1}},"3c07904504e14a4aa99e2a2ccbbc0448":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85f178a458924a0eadb3bb3549bfffae","placeholder":"​","style":"IPY_MODEL_380f9e66449841249a422397cfc51fab","value":" 232k/232k [00:00&lt;00:00, 3.11MB/s]"}},"3ea9a141e33a4a26a86b46c564864556":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ecad6524fff4e24b1e0ee8f4c2013b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fa8a934cb04414ea96b493f804e0524":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffb98fa0f4f4845a3e6f4df529c44c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbf96db2943e4e6c8220770f9d4bbe7e","max":408,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40d8d87016f142f1af91e53eebff98af","value":408}},"400e11e4b0a54ce0a152381c285bc1c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40d8d87016f142f1af91e53eebff98af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4127317775ea4b16a4306dab6a2487e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65da10b3fa8e4ad8b867d9e671c94bc2","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41e1e517b1e44d7da2fe6e410d7974ab","value":28}},"4132989cb9d1483a96c475fdcd36c243":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41e1e517b1e44d7da2fe6e410d7974ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"431e656e24464372ab6fca6724452862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43b185552b07440bab7946a3a7c4fd99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84a669308a114c2da442b961986fdb99","placeholder":"​","style":"IPY_MODEL_a3765ee2b0e84fb3b92fa0b657e9d45a","value":" 6.22k/? [00:00&lt;00:00, 157kB/s]"}},"43c18ffb110441b7899f1e1e80bfc8d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"458085ad133e451fbd334ea65a588923":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7de77b034bf64113bcfb011b1aed5aeb","placeholder":"​","style":"IPY_MODEL_069d2f12ddcf4645bbfb7f30078c847a","value":"Generating train split: 100%"}},"4586527c328e4ae397bde1bfe8b9a59e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_526a174ea90d4842a7d0358356cdfa9d","placeholder":"​","style":"IPY_MODEL_a22216cbb3cc44adabbf253bfae31434","value":" 1725/1725 [00:00&lt;00:00, 12127.80 examples/s]"}},"463a0e09f3044219ae13a0b88709f97b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ed84888dc6842fe896e9a3a67288257","IPY_MODEL_121cdf2eb5fa41c8abc02bc64d8a15d8","IPY_MODEL_854a7a6a4a1b4bd0b9440170796e6351"],"layout":"IPY_MODEL_4f01dc7d37ae4cdda5aaa1cd89221fee"}},"4857cd3df9514f0b8875357717623ef4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bebfda1fb1248e1a60964dea6b93865":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c79638e1c664e6ca7ada2d1faa034b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1beeff4a30b744b193ae46d8d6b588a4","placeholder":"​","style":"IPY_MODEL_efed7fe224594a4985c3fac77b5a8aba","value":" 1725/1725 [00:00&lt;00:00, 4377.53 examples/s]"}},"4cd0b778bd7c489c9ecb3d56a43af724":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f01dc7d37ae4cdda5aaa1cd89221fee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f4f106482fb4991a9017c5871492d8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc6492174fe1495990dd47eaace50d52","IPY_MODEL_10851459e221472296e92b69f1ffdeb8","IPY_MODEL_9a899e01e8234ecf9a3b96483f564f6d"],"layout":"IPY_MODEL_30aeb0512c30413eb8c34e81dd141029"}},"4f7e3c6c9d2c45bfbee63d95335400f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea8c0f8d40d4a24b5d722eae4aacce7","max":1725,"min":0,"orientation":"horizontal","style":"IPY_MODEL_163975a641254219b917214ebe1841c1","value":1725}},"526a174ea90d4842a7d0358356cdfa9d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"532385c63a154c029190d1bc189ef4f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93f735e4079c4ddfa064efce6fb554d4","IPY_MODEL_3ffb98fa0f4f4845a3e6f4df529c44c8","IPY_MODEL_3039526144e548caab131a25789b82b0"],"layout":"IPY_MODEL_d580006fa1d84e04a65bbc31db5675d4"}},"598bb4d4baa1419e81c95cb9072bbab1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"599d5f2098554c4e95aed23e3fa03026":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85883d5affe7419c9b8d7bf07e112f12","IPY_MODEL_cdbe22a341ec4cfc9f0006049080f844","IPY_MODEL_43b185552b07440bab7946a3a7c4fd99"],"layout":"IPY_MODEL_d34133e54dfc4bccb0d43303f2b4a251"}},"5a2695e21c3c485b884c1e35e3ca1bcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aeef2870a1084bcd9b85ecb7163b3d3a","placeholder":"​","style":"IPY_MODEL_0aec5670730744fd9d3a18a361dd4ff7","value":"Downloading builder script: 100%"}},"5d216d7d167840568c2c66f4e44597f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9542f8330de42679fec1e7e5a463883","placeholder":"​","style":"IPY_MODEL_e62e4acd374d4838ac141b32235dfa12","value":"Generating validation split: 100%"}},"5dd1de1bab5c4dc392069e62d0dcec9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6189683c97e643f2830bf915a573b899":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e33bb38880471db7189c1b63574a09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dd73616187e4d83816e2b6276c4107d","placeholder":"​","style":"IPY_MODEL_14728ac7e9be416bbd557dddf904353b","value":" 28.0/28.0 [00:00&lt;00:00, 445B/s]"}},"62e6cc9ad1024590ab26a52d4fb45199":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"637f22bb7ef64f07ad4856a6bc469a54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_458085ad133e451fbd334ea65a588923","IPY_MODEL_22ddde21e3fe47fe92e50c96052d60aa","IPY_MODEL_255506ff5107439abd85b0678a12232b"],"layout":"IPY_MODEL_3fa8a934cb04414ea96b493f804e0524"}},"65ce81a81f9241838de17ce8e53504ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65da10b3fa8e4ad8b867d9e671c94bc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"663099f2b90b43a7967a588f477fa2a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66512a6f542d4d0c9836da12c83b56f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32d70ddf17b54066821f648f90883ca5","placeholder":"​","style":"IPY_MODEL_24611bdc999648fc834d3868ccf629fd","value":"Downloading data: "}},"66920651c5ed4d208788bbd42e18bf40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e765e1455a24f6aa516399fd9f1c886","IPY_MODEL_ade943ec1c7f47cb82da5be6089e4fb4","IPY_MODEL_96aa19d46ae34fd299e3f5675bfa6524"],"layout":"IPY_MODEL_bbf40cc487de4e6fbc9ee7271375aa3a"}},"681b767394f9430899a0b1e215851b06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68a0dc2cf61f4f40af9b539ec1c45536":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69781278612b455cbf6d5c94f25feced":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69b55c5673f34162a40004b6f5b4ceeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b42b798910dd4441bd3a06ce6e4910f6","max":1844,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9454384e86b4ab9be685652afc19990","value":1844}},"6a0390e51455445aa3a85e87ca17abc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a2ed32a0a12459d8ec8ccbd2e5507c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2280552615a4187a06f7e4763f5507d","IPY_MODEL_f105a370e5774d26b2231b21245bdf7c","IPY_MODEL_3c07904504e14a4aa99e2a2ccbbc0448"],"layout":"IPY_MODEL_400e11e4b0a54ce0a152381c285bc1c7"}},"6b479e848c55474797569d48bcce50f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5591dd1654c43ed805f2cfa014d791c","placeholder":"​","style":"IPY_MODEL_004bc5e0b68d4b3f9f2939648092bfee","value":"Downloading metadata: 100%"}},"6e4bb9727ddb445e98e3827dbf4f7993":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4857cd3df9514f0b8875357717623ef4","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8a1b360cbda4e9e961e45fbdc8a235b","value":267954768}},"6e7aed5d24f8444aaf66b080edc5ef35":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"710f97a41f364d169cf7077cd6b5cc1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"711e7b223bfa4745840bcf97707ae79e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4db7766446e42a8b54fbfab637a868d","placeholder":"​","style":"IPY_MODEL_9603c60a79064c3f8e7a58d732cba98d","value":" 28.8k/28.8k [00:00&lt;00:00, 844kB/s]"}},"7194484f386c4b95880b365ea752baf5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91373c6d3278470286c1e67ada83054d","placeholder":"​","style":"IPY_MODEL_9dd81dbb0ad74f6fbc5482a9375fd6a3","value":" 5.76k/? [00:00&lt;00:00, 353kB/s]"}},"71a1db5806a94eacbd9a106117b955e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7241997396bc438d9c87a300d3db344f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"736b774e9609422cb4b50635de7c554f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74f44c22eddd42a7a8afb4b58a4218ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75aca36e6df942cca5052dcaa057bf06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7afeeb7e3fb64861bf994c94c168014e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7de77b034bf64113bcfb011b1aed5aeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ec5dcf538cc41a0ae1d30109b3ee552":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b479e848c55474797569d48bcce50f0","IPY_MODEL_1578e896c7b84d04aef3aa6f5acbeeca","IPY_MODEL_c7016f78951649828d54b826c39d67f7"],"layout":"IPY_MODEL_0f56be71d76b47838f65871a83785a5f"}},"7f74a5f616cf48bf86ec8983ced66d20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af7530e3685a4f10a1f026713af2b0f4","IPY_MODEL_3b0f7a1c720e495488fa395fa796474c","IPY_MODEL_0b0655faf264472785c7041e08534654"],"layout":"IPY_MODEL_69781278612b455cbf6d5c94f25feced"}},"80ed685a43c74043af45f533570b4f29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82fca981fb3641599152bb53030a0286","placeholder":"​","style":"IPY_MODEL_663099f2b90b43a7967a588f477fa2a6","value":" 483/483 [00:00&lt;00:00, 14.2kB/s]"}},"82fca981fb3641599152bb53030a0286":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84517d98e2ad4d86ae40030bbef86b56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4c12c33cf504dec80a1a7acd194a945","placeholder":"​","style":"IPY_MODEL_736b774e9609422cb4b50635de7c554f","value":"Generating test split: 100%"}},"84a669308a114c2da442b961986fdb99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"854a7a6a4a1b4bd0b9440170796e6351":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71a1db5806a94eacbd9a106117b955e1","placeholder":"​","style":"IPY_MODEL_13676e154a0245b7a42d660c2acefe74","value":" 3668/3668 [00:00&lt;00:00, 4369.36 examples/s]"}},"85883d5affe7419c9b8d7bf07e112f12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7241997396bc438d9c87a300d3db344f","placeholder":"​","style":"IPY_MODEL_09fb4d73fc834b0a92359db3f9994ec3","value":"Downloading data: "}},"85f178a458924a0eadb3bb3549bfffae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b0f7aacd5e64c9c8000a9f9466c5a0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bf3c552723e47db8c4e1b3b5b548a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cee5a85e26542358ea241e769a30ddd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e765e1455a24f6aa516399fd9f1c886":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75aca36e6df942cca5052dcaa057bf06","placeholder":"​","style":"IPY_MODEL_62e6cc9ad1024590ab26a52d4fb45199","value":"Downloading data files: 100%"}},"8e7fa04a1a754856a1ead791668be729":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7db178d3e084778903eb652b625d95f","IPY_MODEL_4127317775ea4b16a4306dab6a2487e3","IPY_MODEL_62e33bb38880471db7189c1b63574a09"],"layout":"IPY_MODEL_e612dbde6c364ee393fc1c2a2fd74691"}},"8fa2309779064a5b8df947dac5d0f8cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91373c6d3278470286c1e67ada83054d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92116bcc61004dfba7412cc9b4becc26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66512a6f542d4d0c9836da12c83b56f1","IPY_MODEL_cf03ba3139d84f7eaf7d4cda9bf8f600","IPY_MODEL_c6f25620bf114bf7860e6b5012b37100"],"layout":"IPY_MODEL_eeac69648fe4416ab93eab8286f8d1c2"}},"93f735e4079c4ddfa064efce6fb554d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dd1de1bab5c4dc392069e62d0dcec9b","placeholder":"​","style":"IPY_MODEL_9562eaeb7db04a46829e31ec866890bb","value":"Map: 100%"}},"95475e1e578b42848b7b56196aaccb49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01f243c6f207448999be9f108666d0a0","max":1725,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a7783fb4e33468ca2d92dd752983f0d","value":1725}},"9562eaeb7db04a46829e31ec866890bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9603c60a79064c3f8e7a58d732cba98d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96aa19d46ae34fd299e3f5675bfa6524":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ece466e7c8ad4aa384fc516b02be974c","placeholder":"​","style":"IPY_MODEL_6e7aed5d24f8444aaf66b080edc5ef35","value":" 3/3 [00:01&lt;00:00,  1.97it/s]"}},"9783f588320044059ef1d1339d9f306d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b522e13c07b446678e9c0850ac7d8c46","max":28751,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a0390e51455445aa3a85e87ca17abc7","value":28751}},"9a899e01e8234ecf9a3b96483f564f6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bebfda1fb1248e1a60964dea6b93865","placeholder":"​","style":"IPY_MODEL_f3fe2799a6d64c6ab23492b68f3ee569","value":" 466k/466k [00:00&lt;00:00, 6.61MB/s]"}},"9be79ef6d5144015bec0e70a5d93f349":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d27c79c46d74bb5bba5d3fd280fce30":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9dd73616187e4d83816e2b6276c4107d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd81dbb0ad74f6fbc5482a9375fd6a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a22216cbb3cc44adabbf253bfae31434":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a31b4d419786431da75b7cb13d832f14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a32ee8ad7a4641d786902d681160c2fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a3765ee2b0e84fb3b92fa0b657e9d45a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a56d9d332dcd4c75944c1cfc3bf182a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a865a8cc363147029008ad76c02b707b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9542f8330de42679fec1e7e5a463883":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab0bd56fd10d4e6194871a1b59c78b70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17a68c93c6e94f5b810d9f21673a7114","placeholder":"​","style":"IPY_MODEL_1acff480844e4fb9a684263a340fcf48","value":" 268M/268M [00:01&lt;00:00, 235MB/s]"}},"ab2a03644a52440d9c3f118301a06277":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab5725e298c04d67ad70fd3073a0c420":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad83b367ed6b4069926e86c5150297ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ade943ec1c7f47cb82da5be6089e4fb4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cc2ff419bb5438fb2778b52e203052a","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14c56fb1941e43cdb8adc486817f07bc","value":3}},"ae22dfc3b6b243fc95660b09b173f00b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_166bb8fddc27442b9ccf1018a7a7715f","IPY_MODEL_ed252b47069a496a98595dba61c13750","IPY_MODEL_c9a0c4fc0057459d918c12294b08697b"],"layout":"IPY_MODEL_a31b4d419786431da75b7cb13d832f14"}},"aeef2870a1084bcd9b85ecb7163b3d3a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af7530e3685a4f10a1f026713af2b0f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68a0dc2cf61f4f40af9b539ec1c45536","placeholder":"​","style":"IPY_MODEL_74f44c22eddd42a7a8afb4b58a4218ac","value":"Downloading data: "}},"b42b798910dd4441bd3a06ce6e4910f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4db7766446e42a8b54fbfab637a868d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b522e13c07b446678e9c0850ac7d8c46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b680c5211fbb432d9a159462e5002d52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9851cf025894e5e9d1303d738d85dcf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb78b3609c69474090087ee75cff3dce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbf40cc487de4e6fbc9ee7271375aa3a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be6aee17c20640ed95586fe17013b631":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf7bf27a35ef4dc2b20612017d2f06a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da01b07b55044e6bb220ae78920f1158","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02e3487391a5467686ac798e17f0c7c9","value":483}},"bfc085b6deea454cad3f9d46ee6f429c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c093365d36304e018fbd305a3a66ba71":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c626803332984fddac407d524b8d97ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6f25620bf114bf7860e6b5012b37100":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd29881de199432caefc8beafffc17e2","placeholder":"​","style":"IPY_MODEL_7afeeb7e3fb64861bf994c94c168014e","value":" 1.05M/? [00:00&lt;00:00, 11.9MB/s]"}},"c7016f78951649828d54b826c39d67f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07bd9d9ac50e42dbb60fa8a91a0ba403","placeholder":"​","style":"IPY_MODEL_9d27c79c46d74bb5bba5d3fd280fce30","value":" 28.7k/28.7k [00:00&lt;00:00, 716kB/s]"}},"c7db178d3e084778903eb652b625d95f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df1e2b706a9d44eb852cc2d4c0834daa","placeholder":"​","style":"IPY_MODEL_33952ceb871a46b9bea551be8f9e10b6","value":"Downloading (…)okenizer_config.json: 100%"}},"c89b358605f54e21bc0bd5ccea02b108":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c626803332984fddac407d524b8d97ff","placeholder":"​","style":"IPY_MODEL_df065d7e0a6d4f079148f969d2bafe15","value":"Downloading builder script: "}},"c966d0f0f9834198bd362bbb6c816131":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9a0c4fc0057459d918c12294b08697b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_026b743ebaec4d5ebb1838b6485b01bc","placeholder":"​","style":"IPY_MODEL_25b1b1dd39854d39b12c850142c2785d","value":" 27.9k/27.9k [00:00&lt;00:00, 652kB/s]"}},"cbf96db2943e4e6c8220770f9d4bbe7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdbe22a341ec4cfc9f0006049080f844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6999c2164ac4b80be1e7338e8ad611b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65ce81a81f9241838de17ce8e53504ad","value":1}},"cdd7ef365a0543928fd04e141acfb49d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf03ba3139d84f7eaf7d4cda9bf8f600":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a32ee8ad7a4641d786902d681160c2fa","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07ee38b3498d42f69ede7fb3e8708118","value":1}},"cfa3e95e3b3d4b679aa09d5cf13a5ec9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2280552615a4187a06f7e4763f5507d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e13362715ba54177ac9465a998984ea7","placeholder":"​","style":"IPY_MODEL_2701b457023b4edf80ccc6a98c5ef08d","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"d34133e54dfc4bccb0d43303f2b4a251":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4c12c33cf504dec80a1a7acd194a945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d502bf65b7c24600a0d4a9d85130b05e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d580006fa1d84e04a65bbc31db5675d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6f39e6f87894457b5afa446ec8aa8f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bdf7c6b193441b3a3d5e4b3af4b815d","max":408,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ecad6524fff4e24b1e0ee8f4c2013b9","value":408}},"d99576503ba64499a86048f1428f2e77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff934dd0990d4bd0a873cd88c3fb8a2e","placeholder":"​","style":"IPY_MODEL_ab2a03644a52440d9c3f118301a06277","value":"Map: 100%"}},"da01b07b55044e6bb220ae78920f1158":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd29881de199432caefc8beafffc17e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dea8c0f8d40d4a24b5d722eae4aacce7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df065d7e0a6d4f079148f969d2bafe15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df1e2b706a9d44eb852cc2d4c0834daa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13362715ba54177ac9465a998984ea7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e44c7e8818d540fea169feff2edaf66a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01f7a14072944f508e18c4be851f8d05","placeholder":"​","style":"IPY_MODEL_2ccf61e37cf645feb2552df01f4ca269","value":"Downloading model.safetensors: 100%"}},"e5591dd1654c43ed805f2cfa014d791c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e612dbde6c364ee393fc1c2a2fd74691":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e62e4acd374d4838ac141b32235dfa12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6999c2164ac4b80be1e7338e8ad611b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e9454384e86b4ab9be685652afc19990":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ece466e7c8ad4aa384fc516b02be974c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed252b47069a496a98595dba61c13750":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeac8b2847d74b35a1ff367be762424f","max":27887,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3133415bebd147c6a141ada14746312c","value":27887}},"ee3aea985de24274902c0c35b7c9a486":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39df911a0eac45c78e918d4b387920a5","IPY_MODEL_bf7bf27a35ef4dc2b20612017d2f06a1","IPY_MODEL_80ed685a43c74043af45f533570b4f29"],"layout":"IPY_MODEL_9be79ef6d5144015bec0e70a5d93f349"}},"eeac69648fe4416ab93eab8286f8d1c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeac8b2847d74b35a1ff367be762424f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efed7fe224594a4985c3fac77b5a8aba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f105a370e5774d26b2231b21245bdf7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fa2309779064a5b8df947dac5d0f8cd","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8bf3c552723e47db8c4e1b3b5b548a36","value":231508}},"f3077d90b8764186a245e34fd2399b07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3fe2799a6d64c6ab23492b68f3ee569":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f679b0829856491aac7a1ac55b7cec9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c89b358605f54e21bc0bd5ccea02b108","IPY_MODEL_69b55c5673f34162a40004b6f5b4ceeb","IPY_MODEL_7194484f386c4b95880b365ea752baf5"],"layout":"IPY_MODEL_8cee5a85e26542358ea241e769a30ddd"}},"f77ee1e584c145f3bd2a562b8c958e01":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a2695e21c3c485b884c1e35e3ca1bcf","IPY_MODEL_9783f588320044059ef1d1339d9f306d","IPY_MODEL_711e7b223bfa4745840bcf97707ae79e"],"layout":"IPY_MODEL_b9851cf025894e5e9d1303d738d85dcf"}},"f838dc6ff69b41c682df2af6cc8fd569":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a1b360cbda4e9e961e45fbdc8a235b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc6492174fe1495990dd47eaace50d52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f838dc6ff69b41c682df2af6cc8fd569","placeholder":"​","style":"IPY_MODEL_431e656e24464372ab6fca6724452862","value":"Downloading (…)/main/tokenizer.json: 100%"}},"fed062a5c9604a6c861a4ee89929d77c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff934dd0990d4bd0a873cd88c3fb8a2e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffb2baec7254432f996a45d63db60b57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
